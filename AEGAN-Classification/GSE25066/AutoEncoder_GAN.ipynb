{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a752ea0-b41b-4846-93c0-9ecb3ef9fd93",
   "metadata": {},
   "source": [
    "# 使用AutoEncoder降维"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "02f5b964-8f65-4d51-a52c-5358974c117a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# 定义 AutoEncoder 网络\n",
    "class AutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super(AutoEncoder, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, encoding_dim),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 1024),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(1024, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x\n",
    "\n",
    "# 定义训练函数\n",
    "def train(model, dataloader, num_epochs, learning_rate):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        for data in dataloader:\n",
    "            inputs, _ = data\n",
    "            inputs = inputs.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, inputs)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # 打印损失\n",
    "        if epoch % 100 == 0:\n",
    "            print('Epoch [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, loss.item()))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ebf4a101-63b5-4796-819d-1fbca7430013",
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_normalize(tensor):\n",
    "    min_val = tensor.min()\n",
    "    max_val = tensor.max()\n",
    "    normalized_tensor = (tensor - min_val) / (max_val - min_val)\n",
    "    return normalized_tensor, min_val, max_val\n",
    "\n",
    "def min_max_denormalize(normalized_tensor, min_val, max_val):\n",
    "    denormalized_tensor = normalized_tensor * (max_val - min_val) + min_val\n",
    "    return denormalized_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9038f326-d088-4a72-841c-d9fdec7382e0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def setup_seed(seed):\n",
    "     torch.manual_seed(seed)\n",
    "     torch.cuda.manual_seed_all(seed)\n",
    "     torch.cuda.manual_seed(seed)\n",
    "     np.random.seed(seed)\n",
    "     torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "510af8bf-d652-40ac-bcef-f175568c560f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup_seed(42)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e1e3c2f-136e-43f6-b122-c4711e58845e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.6131, 0.5153, 0.4467,  ..., 0.4749, 0.3460, 0.4236],\n",
      "        [0.6731, 0.4737, 0.4691,  ..., 0.4936, 0.3113, 0.5068],\n",
      "        [0.6626, 0.4508, 0.4821,  ..., 0.4458, 0.4158, 0.4828],\n",
      "        ...,\n",
      "        [0.6257, 0.5087, 0.4559,  ..., 0.4788, 0.3211, 0.4210],\n",
      "        [0.6404, 0.4866, 0.4323,  ..., 0.4856, 0.2900, 0.4329],\n",
      "        [0.6162, 0.4467, 0.4790,  ..., 0.4762, 0.3068, 0.4674]],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# 加载数据\n",
    "data = pd.read_csv('GSE25066_merge.csv')\n",
    "data = data[data['group'] == 0]\n",
    "data.pop('group')\n",
    "data_tensor = torch.from_numpy(data.values).to(torch.float32).to(device)\n",
    "nor_data_tensor, ae_min,ae_max = min_max_normalize(data_tensor)\n",
    "print(nor_data_tensor)\n",
    "dataset = TensorDataset(nor_data_tensor,nor_data_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# 创建 AutoEncoder 模型\n",
    "input_dim = 13236\n",
    "encoding_dim = 512  # 降维后的维度\n",
    "model = AutoEncoder(input_dim, encoding_dim).to(device)\n",
    "\n",
    "# 训练模型\n",
    "num_epochs = 10\n",
    "learning_rate = 0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2d17a936-ea59-4ec7-b770-75f22691266b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GSE20194_AE.pth:not exist!\n",
      "Epoch [1/1000], Loss: 0.0110\n",
      "Epoch [101/1000], Loss: 0.0015\n",
      "Epoch [201/1000], Loss: 0.0019\n",
      "Epoch [301/1000], Loss: 0.0011\n",
      "Epoch [401/1000], Loss: 0.0014\n",
      "Epoch [501/1000], Loss: 0.0008\n",
      "Epoch [601/1000], Loss: 0.0011\n",
      "Epoch [701/1000], Loss: 0.0010\n",
      "Epoch [801/1000], Loss: 0.0007\n",
      "Epoch [901/1000], Loss: 0.0006\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "model_file_name = 'GSE20194_AE.pth'\n",
    "model_file = Path(model_file_name)\n",
    "if model_file.exists():\n",
    "    # 指定的文件存在\n",
    "    print(f'{model_file_name}:read model params!')\n",
    "    model.load_state_dict(torch.load(model_file_name))\n",
    "else:\n",
    "    print(f'{model_file_name}:not exist!')\n",
    "    train(model, dataloader, 1000, learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8c128d48-598f-4097-bb20-bf918de4251e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用训练好的模型对数据进行降维\n",
    "encoded_data = model.encoder(nor_data_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "108b1eec-5840-47f2-b300-9848f9fa30c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6131, 0.5153, 0.4467,  ..., 0.4749, 0.3460, 0.4236],\n",
       "        [0.6731, 0.4737, 0.4691,  ..., 0.4936, 0.3113, 0.5068],\n",
       "        [0.6626, 0.4508, 0.4821,  ..., 0.4458, 0.4158, 0.4828],\n",
       "        ...,\n",
       "        [0.6257, 0.5087, 0.4559,  ..., 0.4788, 0.3211, 0.4210],\n",
       "        [0.6404, 0.4866, 0.4323,  ..., 0.4856, 0.2900, 0.4329],\n",
       "        [0.6162, 0.4467, 0.4790,  ..., 0.4762, 0.3068, 0.4674]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nor_data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "deb8bebe-5270-4f1b-8f84-0858c61c8f8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.8287,  9.5239,  7.9073,  ...,  8.5732,  5.5354,  7.3647],\n",
       "        [13.2413,  8.5433,  8.4357,  ...,  9.0120,  4.7177,  9.3233],\n",
       "        [12.9936,  8.0050,  8.7417,  ...,  7.8872,  7.1810,  8.7589],\n",
       "        ...,\n",
       "        [12.1239,  9.3687,  8.1248,  ...,  8.6634,  4.9483,  7.3026],\n",
       "        [12.4712,  8.8489,  7.5696,  ...,  8.8234,  4.2163,  7.5835],\n",
       "        [11.9005,  7.9081,  8.6694,  ...,  8.6022,  4.6114,  8.3969]],\n",
       "       device='cuda:0')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fe16ea08-6974-4156-81dc-c5f7afdcbb1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.5095, 0.9459, 0.0000,  ..., 0.0000, 0.0000, 0.8093],\n",
       "        [1.7483, 0.0000, 0.0000,  ..., 0.0000, 0.0000, 2.0990],\n",
       "        [0.2235, 0.6540, 0.0000,  ..., 0.0000, 0.0000, 1.1112],\n",
       "        ...,\n",
       "        [0.7289, 1.9455, 0.0000,  ..., 0.0000, 0.0000, 0.7178],\n",
       "        [0.4070, 1.1969, 0.0000,  ..., 0.0000, 0.0000, 0.1429],\n",
       "        [0.3198, 0.2224, 0.0000,  ..., 0.0000, 0.0000, 0.1588]],\n",
       "       device='cuda:0', grad_fn=<ReluBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5161a51b-2c4f-4169-8a94-7480e373c233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.6173, 0.5034, 0.4290,  ..., 0.4702, 0.3797, 0.4209],\n",
       "        [0.6746, 0.4700, 0.4597,  ..., 0.5012, 0.3022, 0.5044],\n",
       "        [0.6533, 0.4342, 0.4575,  ..., 0.4666, 0.3960, 0.4833],\n",
       "        ...,\n",
       "        [0.6149, 0.4996, 0.4395,  ..., 0.4764, 0.3979, 0.4400],\n",
       "        [0.6242, 0.4772, 0.4343,  ..., 0.4845, 0.3158, 0.4413],\n",
       "        [0.6120, 0.4552, 0.4642,  ..., 0.4797, 0.3144, 0.4534]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.decoder(encoded_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d3740d31-db86-487a-8705-7d0a702584fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[11.9266,  9.2438,  7.4902,  ...,  8.4627,  6.3296,  7.3013],\n",
       "        [13.2775,  8.4568,  8.2137,  ...,  9.1930,  4.5050,  9.2669],\n",
       "        [12.7740,  7.6133,  8.1622,  ...,  8.3760,  6.7132,  8.7697],\n",
       "        ...,\n",
       "        [11.8704,  9.1547,  7.7390,  ...,  8.6067,  6.7595,  7.7507],\n",
       "        [12.0904,  8.6265,  7.6169,  ...,  8.7994,  4.8251,  7.7815],\n",
       "        [11.8020,  8.1084,  8.3207,  ...,  8.6859,  4.7912,  8.0670]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_data_tensor = min_max_denormalize(model.decoder(encoded_data),ae_min,ae_max)\n",
    "decoder_data_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2a7c556b-253b-4c05-a9bb-e3d1a646382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('GSE25066_AutoEncoder.npz', origin_data=data_tensor.cpu().detach().numpy(), decoder_data=decoder_data_tensor.cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "22d45666-5bb4-47b7-bccc-fd90604ca905",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'GSE25066_AE.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db7b531-83e7-426b-8eb1-d307e9a059fe",
   "metadata": {},
   "source": [
    "# 生成对抗网络"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "13eaa3f1-6839-4aa2-bf07-0452824e310b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# 定义生成器（Generator）\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(Generator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 定义判别器（Discriminator）\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "# 超参数设置\n",
    "input_dim = 64\n",
    "data_dim = 512\n",
    "lr = 0.00001\n",
    "epochs = 1000\n",
    "batch_size = 64\n",
    "\n",
    "# 初始化生成器和判别器\n",
    "generator = Generator(input_dim, data_dim).to(device)\n",
    "discriminator = Discriminator(data_dim).to(device)\n",
    "\n",
    "# 设置优化器\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=lr)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=lr)\n",
    "\n",
    "# 设置损失函数\n",
    "loss_func = nn.BCELoss()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "329447ed-1b37-43f1-928b-f1c58f9883f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([99, 512])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "cb17489e-2549-4368-909d-53763aaeb7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_dataset = TensorDataset(encoded_data,encoded_data)\n",
    "gan_dataloader = DataLoader(gan_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "90a6c881-80b9-4287-be97-0fd0a64ea244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, G_Loss: 0.9707, D_Loss: 0.6754\n",
      "Epoch: 10, G_Loss: 0.9754, D_Loss: 0.6703\n",
      "Epoch: 20, G_Loss: 0.9696, D_Loss: 0.6750\n",
      "Epoch: 30, G_Loss: 0.9611, D_Loss: 0.6673\n",
      "Epoch: 40, G_Loss: 0.9633, D_Loss: 0.6657\n",
      "Epoch: 50, G_Loss: 0.9875, D_Loss: 0.6387\n",
      "Epoch: 60, G_Loss: 1.0109, D_Loss: 0.6408\n",
      "Epoch: 70, G_Loss: 1.0257, D_Loss: 0.6160\n",
      "Epoch: 80, G_Loss: 1.0285, D_Loss: 0.6199\n",
      "Epoch: 90, G_Loss: 1.0372, D_Loss: 0.6135\n",
      "Epoch: 100, G_Loss: 1.0311, D_Loss: 0.6098\n",
      "Epoch: 110, G_Loss: 1.0342, D_Loss: 0.6110\n",
      "Epoch: 120, G_Loss: 1.0213, D_Loss: 0.6399\n",
      "Epoch: 130, G_Loss: 0.9965, D_Loss: 0.6532\n",
      "Epoch: 140, G_Loss: 0.9669, D_Loss: 0.6583\n",
      "Epoch: 150, G_Loss: 0.9645, D_Loss: 0.6690\n",
      "Epoch: 160, G_Loss: 0.9429, D_Loss: 0.6961\n",
      "Epoch: 170, G_Loss: 0.9473, D_Loss: 0.6651\n",
      "Epoch: 180, G_Loss: 0.9855, D_Loss: 0.6481\n",
      "Epoch: 190, G_Loss: 1.0328, D_Loss: 0.6213\n",
      "Epoch: 200, G_Loss: 1.0459, D_Loss: 0.6050\n",
      "Epoch: 210, G_Loss: 1.0924, D_Loss: 0.5786\n",
      "Epoch: 220, G_Loss: 1.0945, D_Loss: 0.5873\n",
      "Epoch: 230, G_Loss: 1.1252, D_Loss: 0.5713\n",
      "Epoch: 240, G_Loss: 1.1386, D_Loss: 0.5482\n",
      "Epoch: 250, G_Loss: 1.1446, D_Loss: 0.5391\n",
      "Epoch: 260, G_Loss: 1.1449, D_Loss: 0.5258\n",
      "Epoch: 270, G_Loss: 1.1430, D_Loss: 0.5433\n",
      "Epoch: 280, G_Loss: 1.1572, D_Loss: 0.5454\n",
      "Epoch: 290, G_Loss: 1.1202, D_Loss: 0.5604\n",
      "Epoch: 300, G_Loss: 1.1154, D_Loss: 0.5850\n",
      "Epoch: 310, G_Loss: 1.1023, D_Loss: 0.5813\n",
      "Epoch: 320, G_Loss: 1.0665, D_Loss: 0.5900\n",
      "Epoch: 330, G_Loss: 1.0730, D_Loss: 0.6093\n",
      "Epoch: 340, G_Loss: 1.0596, D_Loss: 0.6242\n",
      "Epoch: 350, G_Loss: 1.0840, D_Loss: 0.6040\n",
      "Epoch: 360, G_Loss: 1.1285, D_Loss: 0.5806\n",
      "Epoch: 370, G_Loss: 1.1297, D_Loss: 0.5583\n",
      "Epoch: 380, G_Loss: 1.1556, D_Loss: 0.5579\n",
      "Epoch: 390, G_Loss: 1.1630, D_Loss: 0.5538\n",
      "Epoch: 400, G_Loss: 1.1532, D_Loss: 0.5438\n",
      "Epoch: 410, G_Loss: 1.1402, D_Loss: 0.5422\n",
      "Epoch: 420, G_Loss: 1.1526, D_Loss: 0.5486\n",
      "Epoch: 430, G_Loss: 1.1653, D_Loss: 0.5584\n",
      "Epoch: 440, G_Loss: 1.1928, D_Loss: 0.5324\n",
      "Epoch: 450, G_Loss: 1.1908, D_Loss: 0.5193\n",
      "Epoch: 460, G_Loss: 1.2153, D_Loss: 0.4911\n",
      "Epoch: 470, G_Loss: 1.2407, D_Loss: 0.4998\n",
      "Epoch: 480, G_Loss: 1.2566, D_Loss: 0.4800\n",
      "Epoch: 490, G_Loss: 1.2740, D_Loss: 0.4943\n",
      "Epoch: 500, G_Loss: 1.2679, D_Loss: 0.4768\n",
      "Epoch: 510, G_Loss: 1.2911, D_Loss: 0.4786\n",
      "Epoch: 520, G_Loss: 1.3075, D_Loss: 0.4589\n",
      "Epoch: 530, G_Loss: 1.2744, D_Loss: 0.4648\n",
      "Epoch: 540, G_Loss: 1.2644, D_Loss: 0.4805\n",
      "Epoch: 550, G_Loss: 1.2634, D_Loss: 0.4905\n",
      "Epoch: 560, G_Loss: 1.2670, D_Loss: 0.4799\n",
      "Epoch: 570, G_Loss: 1.2871, D_Loss: 0.4772\n",
      "Epoch: 580, G_Loss: 1.3037, D_Loss: 0.4753\n",
      "Epoch: 590, G_Loss: 1.3329, D_Loss: 0.4282\n",
      "Epoch: 600, G_Loss: 1.3485, D_Loss: 0.4314\n",
      "Epoch: 610, G_Loss: 1.3685, D_Loss: 0.4406\n",
      "Epoch: 620, G_Loss: 1.3844, D_Loss: 0.4154\n",
      "Epoch: 630, G_Loss: 1.3939, D_Loss: 0.4227\n",
      "Epoch: 640, G_Loss: 1.3949, D_Loss: 0.4307\n",
      "Epoch: 650, G_Loss: 1.4065, D_Loss: 0.4077\n",
      "Epoch: 660, G_Loss: 1.4448, D_Loss: 0.3910\n",
      "Epoch: 670, G_Loss: 1.4582, D_Loss: 0.3745\n",
      "Epoch: 680, G_Loss: 1.4738, D_Loss: 0.3793\n",
      "Epoch: 690, G_Loss: 1.4441, D_Loss: 0.3944\n",
      "Epoch: 700, G_Loss: 1.4229, D_Loss: 0.3842\n",
      "Epoch: 710, G_Loss: 1.3895, D_Loss: 0.4003\n",
      "Epoch: 720, G_Loss: 1.3616, D_Loss: 0.4194\n",
      "Epoch: 730, G_Loss: 1.3688, D_Loss: 0.4555\n",
      "Epoch: 740, G_Loss: 1.3478, D_Loss: 0.4111\n",
      "Epoch: 750, G_Loss: 1.3547, D_Loss: 0.4440\n",
      "Epoch: 760, G_Loss: 1.3495, D_Loss: 0.4363\n",
      "Epoch: 770, G_Loss: 1.3320, D_Loss: 0.4395\n",
      "Epoch: 780, G_Loss: 1.3699, D_Loss: 0.4368\n",
      "Epoch: 790, G_Loss: 1.3713, D_Loss: 0.4231\n",
      "Epoch: 800, G_Loss: 1.4028, D_Loss: 0.4205\n",
      "Epoch: 810, G_Loss: 1.3973, D_Loss: 0.4123\n",
      "Epoch: 820, G_Loss: 1.4263, D_Loss: 0.4245\n",
      "Epoch: 830, G_Loss: 1.4420, D_Loss: 0.4011\n",
      "Epoch: 840, G_Loss: 1.4713, D_Loss: 0.4012\n",
      "Epoch: 850, G_Loss: 1.4746, D_Loss: 0.3591\n",
      "Epoch: 860, G_Loss: 1.4858, D_Loss: 0.3682\n",
      "Epoch: 870, G_Loss: 1.4933, D_Loss: 0.3773\n",
      "Epoch: 880, G_Loss: 1.5393, D_Loss: 0.3648\n",
      "Epoch: 890, G_Loss: 1.5545, D_Loss: 0.3691\n",
      "Epoch: 900, G_Loss: 1.5866, D_Loss: 0.3351\n",
      "Epoch: 910, G_Loss: 1.5768, D_Loss: 0.3183\n",
      "Epoch: 920, G_Loss: 1.5884, D_Loss: 0.3290\n",
      "Epoch: 930, G_Loss: 1.5888, D_Loss: 0.3143\n",
      "Epoch: 940, G_Loss: 1.6640, D_Loss: 0.2920\n",
      "Epoch: 950, G_Loss: 1.6656, D_Loss: 0.2960\n",
      "Epoch: 960, G_Loss: 1.6610, D_Loss: 0.3091\n",
      "Epoch: 970, G_Loss: 1.6114, D_Loss: 0.3173\n",
      "Epoch: 980, G_Loss: 1.6361, D_Loss: 0.3184\n",
      "Epoch: 990, G_Loss: 1.6216, D_Loss: 0.3220\n",
      "Epoch: 1000, G_Loss: 1.5901, D_Loss: 0.3371\n",
      "Epoch: 1010, G_Loss: 1.5793, D_Loss: 0.3616\n",
      "Epoch: 1020, G_Loss: 1.5828, D_Loss: 0.3421\n",
      "Epoch: 1030, G_Loss: 1.6168, D_Loss: 0.3153\n",
      "Epoch: 1040, G_Loss: 1.6599, D_Loss: 0.3238\n",
      "Epoch: 1050, G_Loss: 1.6111, D_Loss: 0.3320\n",
      "Epoch: 1060, G_Loss: 1.6427, D_Loss: 0.3341\n",
      "Epoch: 1070, G_Loss: 1.6638, D_Loss: 0.3163\n",
      "Epoch: 1080, G_Loss: 1.6642, D_Loss: 0.3097\n",
      "Epoch: 1090, G_Loss: 1.6850, D_Loss: 0.2839\n",
      "Epoch: 1100, G_Loss: 1.7178, D_Loss: 0.2936\n",
      "Epoch: 1110, G_Loss: 1.7547, D_Loss: 0.2933\n",
      "Epoch: 1120, G_Loss: 1.7656, D_Loss: 0.2608\n",
      "Epoch: 1130, G_Loss: 1.7459, D_Loss: 0.2758\n",
      "Epoch: 1140, G_Loss: 1.7710, D_Loss: 0.2674\n",
      "Epoch: 1150, G_Loss: 1.7773, D_Loss: 0.2928\n",
      "Epoch: 1160, G_Loss: 1.7916, D_Loss: 0.2545\n",
      "Epoch: 1170, G_Loss: 1.8218, D_Loss: 0.2745\n",
      "Epoch: 1180, G_Loss: 1.8675, D_Loss: 0.2627\n",
      "Epoch: 1190, G_Loss: 1.8772, D_Loss: 0.2423\n",
      "Epoch: 1200, G_Loss: 1.9159, D_Loss: 0.2433\n",
      "Epoch: 1210, G_Loss: 1.8935, D_Loss: 0.2608\n",
      "Epoch: 1220, G_Loss: 1.8661, D_Loss: 0.2350\n",
      "Epoch: 1230, G_Loss: 1.8824, D_Loss: 0.2426\n",
      "Epoch: 1240, G_Loss: 1.8876, D_Loss: 0.2307\n",
      "Epoch: 1250, G_Loss: 1.9045, D_Loss: 0.2429\n",
      "Epoch: 1260, G_Loss: 1.8639, D_Loss: 0.2547\n",
      "Epoch: 1270, G_Loss: 1.8999, D_Loss: 0.2408\n",
      "Epoch: 1280, G_Loss: 1.8243, D_Loss: 0.2581\n",
      "Epoch: 1290, G_Loss: 1.8520, D_Loss: 0.2624\n",
      "Epoch: 1300, G_Loss: 1.8257, D_Loss: 0.2585\n",
      "Epoch: 1310, G_Loss: 1.9003, D_Loss: 0.2515\n",
      "Epoch: 1320, G_Loss: 1.8813, D_Loss: 0.2425\n",
      "Epoch: 1330, G_Loss: 1.8805, D_Loss: 0.2438\n",
      "Epoch: 1340, G_Loss: 1.8836, D_Loss: 0.2380\n",
      "Epoch: 1350, G_Loss: 1.9650, D_Loss: 0.2381\n",
      "Epoch: 1360, G_Loss: 1.9946, D_Loss: 0.2363\n",
      "Epoch: 1370, G_Loss: 1.9974, D_Loss: 0.2108\n",
      "Epoch: 1380, G_Loss: 2.0459, D_Loss: 0.2231\n",
      "Epoch: 1390, G_Loss: 2.0024, D_Loss: 0.2153\n",
      "Epoch: 1400, G_Loss: 2.0911, D_Loss: 0.2130\n",
      "Epoch: 1410, G_Loss: 2.0721, D_Loss: 0.2079\n",
      "Epoch: 1420, G_Loss: 2.0907, D_Loss: 0.1976\n",
      "Epoch: 1430, G_Loss: 2.0310, D_Loss: 0.2061\n",
      "Epoch: 1440, G_Loss: 2.0322, D_Loss: 0.2073\n",
      "Epoch: 1450, G_Loss: 1.9858, D_Loss: 0.2312\n",
      "Epoch: 1460, G_Loss: 1.9313, D_Loss: 0.2264\n",
      "Epoch: 1470, G_Loss: 2.0135, D_Loss: 0.2318\n",
      "Epoch: 1480, G_Loss: 2.0219, D_Loss: 0.2240\n",
      "Epoch: 1490, G_Loss: 2.0275, D_Loss: 0.2104\n",
      "Epoch: 1500, G_Loss: 2.0710, D_Loss: 0.2077\n",
      "Epoch: 1510, G_Loss: 2.1277, D_Loss: 0.1986\n",
      "Epoch: 1520, G_Loss: 2.1613, D_Loss: 0.1914\n",
      "Epoch: 1530, G_Loss: 2.1320, D_Loss: 0.1730\n",
      "Epoch: 1540, G_Loss: 2.2215, D_Loss: 0.1592\n",
      "Epoch: 1550, G_Loss: 2.2196, D_Loss: 0.1612\n",
      "Epoch: 1560, G_Loss: 2.2668, D_Loss: 0.1703\n",
      "Epoch: 1570, G_Loss: 2.2724, D_Loss: 0.1624\n",
      "Epoch: 1580, G_Loss: 2.2604, D_Loss: 0.1731\n",
      "Epoch: 1590, G_Loss: 2.2708, D_Loss: 0.1682\n",
      "Epoch: 1600, G_Loss: 2.2567, D_Loss: 0.1866\n",
      "Epoch: 1610, G_Loss: 2.2522, D_Loss: 0.1756\n",
      "Epoch: 1620, G_Loss: 2.1783, D_Loss: 0.1799\n",
      "Epoch: 1630, G_Loss: 2.1767, D_Loss: 0.1813\n",
      "Epoch: 1640, G_Loss: 2.1138, D_Loss: 0.1885\n",
      "Epoch: 1650, G_Loss: 2.1629, D_Loss: 0.1960\n",
      "Epoch: 1660, G_Loss: 2.1511, D_Loss: 0.1794\n",
      "Epoch: 1670, G_Loss: 2.1851, D_Loss: 0.1838\n",
      "Epoch: 1680, G_Loss: 2.2371, D_Loss: 0.1720\n",
      "Epoch: 1690, G_Loss: 2.2416, D_Loss: 0.1638\n",
      "Epoch: 1700, G_Loss: 2.3337, D_Loss: 0.1690\n",
      "Epoch: 1710, G_Loss: 2.3290, D_Loss: 0.1540\n",
      "Epoch: 1720, G_Loss: 2.3418, D_Loss: 0.1456\n",
      "Epoch: 1730, G_Loss: 2.3191, D_Loss: 0.1476\n",
      "Epoch: 1740, G_Loss: 2.3295, D_Loss: 0.1587\n",
      "Epoch: 1750, G_Loss: 2.2867, D_Loss: 0.1587\n",
      "Epoch: 1760, G_Loss: 2.3262, D_Loss: 0.1608\n",
      "Epoch: 1770, G_Loss: 2.3188, D_Loss: 0.1494\n",
      "Epoch: 1780, G_Loss: 2.2909, D_Loss: 0.1509\n",
      "Epoch: 1790, G_Loss: 2.3498, D_Loss: 0.1539\n",
      "Epoch: 1800, G_Loss: 2.3761, D_Loss: 0.1623\n",
      "Epoch: 1810, G_Loss: 2.3564, D_Loss: 0.1587\n",
      "Epoch: 1820, G_Loss: 2.3395, D_Loss: 0.1526\n",
      "Epoch: 1830, G_Loss: 2.3970, D_Loss: 0.1456\n",
      "Epoch: 1840, G_Loss: 2.4120, D_Loss: 0.1353\n",
      "Epoch: 1850, G_Loss: 2.3557, D_Loss: 0.1451\n",
      "Epoch: 1860, G_Loss: 2.3705, D_Loss: 0.1484\n",
      "Epoch: 1870, G_Loss: 2.3762, D_Loss: 0.1463\n",
      "Epoch: 1880, G_Loss: 2.3942, D_Loss: 0.1476\n",
      "Epoch: 1890, G_Loss: 2.4371, D_Loss: 0.1441\n",
      "Epoch: 1900, G_Loss: 2.4450, D_Loss: 0.1355\n",
      "Epoch: 1910, G_Loss: 2.4766, D_Loss: 0.1223\n",
      "Epoch: 1920, G_Loss: 2.5436, D_Loss: 0.1332\n",
      "Epoch: 1930, G_Loss: 2.5146, D_Loss: 0.1170\n",
      "Epoch: 1940, G_Loss: 2.5121, D_Loss: 0.1266\n",
      "Epoch: 1950, G_Loss: 2.4975, D_Loss: 0.1409\n",
      "Epoch: 1960, G_Loss: 2.4551, D_Loss: 0.1338\n",
      "Epoch: 1970, G_Loss: 2.4704, D_Loss: 0.1430\n",
      "Epoch: 1980, G_Loss: 2.4594, D_Loss: 0.1416\n",
      "Epoch: 1990, G_Loss: 2.4812, D_Loss: 0.1330\n",
      "Epoch: 2000, G_Loss: 2.5128, D_Loss: 0.1409\n",
      "Epoch: 2010, G_Loss: 2.5974, D_Loss: 0.1285\n",
      "Epoch: 2020, G_Loss: 2.5423, D_Loss: 0.1350\n",
      "Epoch: 2030, G_Loss: 2.5735, D_Loss: 0.1206\n",
      "Epoch: 2040, G_Loss: 2.6145, D_Loss: 0.1183\n",
      "Epoch: 2050, G_Loss: 2.6723, D_Loss: 0.1243\n",
      "Epoch: 2060, G_Loss: 2.7769, D_Loss: 0.1020\n",
      "Epoch: 2070, G_Loss: 2.7773, D_Loss: 0.0911\n",
      "Epoch: 2080, G_Loss: 2.8382, D_Loss: 0.0988\n",
      "Epoch: 2090, G_Loss: 2.8055, D_Loss: 0.0971\n",
      "Epoch: 2100, G_Loss: 2.7485, D_Loss: 0.0987\n",
      "Epoch: 2110, G_Loss: 2.7806, D_Loss: 0.1058\n",
      "Epoch: 2120, G_Loss: 2.7576, D_Loss: 0.1126\n",
      "Epoch: 2130, G_Loss: 2.7023, D_Loss: 0.1053\n",
      "Epoch: 2140, G_Loss: 2.7654, D_Loss: 0.1106\n",
      "Epoch: 2150, G_Loss: 2.7221, D_Loss: 0.1002\n",
      "Epoch: 2160, G_Loss: 2.6698, D_Loss: 0.1043\n",
      "Epoch: 2170, G_Loss: 2.6719, D_Loss: 0.1041\n",
      "Epoch: 2180, G_Loss: 2.7296, D_Loss: 0.1032\n",
      "Epoch: 2190, G_Loss: 2.6904, D_Loss: 0.1064\n",
      "Epoch: 2200, G_Loss: 2.8339, D_Loss: 0.1000\n",
      "Epoch: 2210, G_Loss: 2.7981, D_Loss: 0.0892\n",
      "Epoch: 2220, G_Loss: 2.8665, D_Loss: 0.0900\n",
      "Epoch: 2230, G_Loss: 2.8159, D_Loss: 0.0909\n",
      "Epoch: 2240, G_Loss: 2.8207, D_Loss: 0.0939\n",
      "Epoch: 2250, G_Loss: 2.8363, D_Loss: 0.0905\n",
      "Epoch: 2260, G_Loss: 2.8384, D_Loss: 0.0845\n",
      "Epoch: 2270, G_Loss: 2.8815, D_Loss: 0.0889\n",
      "Epoch: 2280, G_Loss: 2.8811, D_Loss: 0.0891\n",
      "Epoch: 2290, G_Loss: 2.8753, D_Loss: 0.0895\n",
      "Epoch: 2300, G_Loss: 2.8678, D_Loss: 0.0937\n",
      "Epoch: 2310, G_Loss: 2.8442, D_Loss: 0.0915\n",
      "Epoch: 2320, G_Loss: 2.8451, D_Loss: 0.0882\n",
      "Epoch: 2330, G_Loss: 2.8304, D_Loss: 0.0946\n",
      "Epoch: 2340, G_Loss: 2.8543, D_Loss: 0.0914\n",
      "Epoch: 2350, G_Loss: 2.8664, D_Loss: 0.0884\n",
      "Epoch: 2360, G_Loss: 2.8529, D_Loss: 0.0944\n",
      "Epoch: 2370, G_Loss: 2.9000, D_Loss: 0.0968\n",
      "Epoch: 2380, G_Loss: 2.8746, D_Loss: 0.0784\n",
      "Epoch: 2390, G_Loss: 2.9294, D_Loss: 0.0806\n",
      "Epoch: 2400, G_Loss: 3.0198, D_Loss: 0.0810\n",
      "Epoch: 2410, G_Loss: 3.0329, D_Loss: 0.0745\n",
      "Epoch: 2420, G_Loss: 3.0979, D_Loss: 0.0739\n",
      "Epoch: 2430, G_Loss: 3.0787, D_Loss: 0.0715\n",
      "Epoch: 2440, G_Loss: 3.0758, D_Loss: 0.0690\n",
      "Epoch: 2450, G_Loss: 3.0331, D_Loss: 0.0721\n",
      "Epoch: 2460, G_Loss: 3.0365, D_Loss: 0.0712\n",
      "Epoch: 2470, G_Loss: 3.0437, D_Loss: 0.0745\n",
      "Epoch: 2480, G_Loss: 3.1223, D_Loss: 0.0677\n",
      "Epoch: 2490, G_Loss: 3.1495, D_Loss: 0.0748\n",
      "Epoch: 2500, G_Loss: 3.2206, D_Loss: 0.0686\n",
      "Epoch: 2510, G_Loss: 3.1655, D_Loss: 0.0690\n",
      "Epoch: 2520, G_Loss: 3.1893, D_Loss: 0.0614\n",
      "Epoch: 2530, G_Loss: 3.2184, D_Loss: 0.0660\n",
      "Epoch: 2540, G_Loss: 3.2204, D_Loss: 0.0637\n",
      "Epoch: 2550, G_Loss: 3.2052, D_Loss: 0.0624\n",
      "Epoch: 2560, G_Loss: 3.2417, D_Loss: 0.0724\n",
      "Epoch: 2570, G_Loss: 3.1930, D_Loss: 0.0636\n",
      "Epoch: 2580, G_Loss: 3.2012, D_Loss: 0.0646\n",
      "Epoch: 2590, G_Loss: 3.2470, D_Loss: 0.0616\n",
      "Epoch: 2600, G_Loss: 3.2726, D_Loss: 0.0622\n",
      "Epoch: 2610, G_Loss: 3.2576, D_Loss: 0.0649\n",
      "Epoch: 2620, G_Loss: 3.2908, D_Loss: 0.0626\n",
      "Epoch: 2630, G_Loss: 3.2749, D_Loss: 0.0571\n",
      "Epoch: 2640, G_Loss: 3.3381, D_Loss: 0.0604\n",
      "Epoch: 2650, G_Loss: 3.3283, D_Loss: 0.0555\n",
      "Epoch: 2660, G_Loss: 3.2705, D_Loss: 0.0603\n",
      "Epoch: 2670, G_Loss: 3.2585, D_Loss: 0.0633\n",
      "Epoch: 2680, G_Loss: 3.2964, D_Loss: 0.0571\n",
      "Epoch: 2690, G_Loss: 3.2836, D_Loss: 0.0612\n",
      "Epoch: 2700, G_Loss: 3.3388, D_Loss: 0.0636\n",
      "Epoch: 2710, G_Loss: 3.3166, D_Loss: 0.0613\n",
      "Epoch: 2720, G_Loss: 3.2742, D_Loss: 0.0598\n",
      "Epoch: 2730, G_Loss: 3.3402, D_Loss: 0.0592\n",
      "Epoch: 2740, G_Loss: 3.3756, D_Loss: 0.0546\n",
      "Epoch: 2750, G_Loss: 3.3756, D_Loss: 0.0512\n",
      "Epoch: 2760, G_Loss: 3.4158, D_Loss: 0.0521\n",
      "Epoch: 2770, G_Loss: 3.4085, D_Loss: 0.0542\n",
      "Epoch: 2780, G_Loss: 3.4876, D_Loss: 0.0498\n",
      "Epoch: 2790, G_Loss: 3.5200, D_Loss: 0.0455\n",
      "Epoch: 2800, G_Loss: 3.5876, D_Loss: 0.0447\n",
      "Epoch: 2810, G_Loss: 3.5652, D_Loss: 0.0466\n",
      "Epoch: 2820, G_Loss: 3.5984, D_Loss: 0.0440\n",
      "Epoch: 2830, G_Loss: 3.5548, D_Loss: 0.0436\n",
      "Epoch: 2840, G_Loss: 3.5466, D_Loss: 0.0443\n",
      "Epoch: 2850, G_Loss: 3.4646, D_Loss: 0.0476\n",
      "Epoch: 2860, G_Loss: 3.5321, D_Loss: 0.0505\n",
      "Epoch: 2870, G_Loss: 3.5172, D_Loss: 0.0455\n",
      "Epoch: 2880, G_Loss: 3.5323, D_Loss: 0.0467\n",
      "Epoch: 2890, G_Loss: 3.5499, D_Loss: 0.0481\n",
      "Epoch: 2900, G_Loss: 3.5678, D_Loss: 0.0448\n",
      "Epoch: 2910, G_Loss: 3.5970, D_Loss: 0.0446\n",
      "Epoch: 2920, G_Loss: 3.5355, D_Loss: 0.0439\n",
      "Epoch: 2930, G_Loss: 3.6205, D_Loss: 0.0437\n",
      "Epoch: 2940, G_Loss: 3.5576, D_Loss: 0.0415\n",
      "Epoch: 2950, G_Loss: 3.5800, D_Loss: 0.0462\n",
      "Epoch: 2960, G_Loss: 3.6735, D_Loss: 0.0426\n",
      "Epoch: 2970, G_Loss: 3.7057, D_Loss: 0.0425\n",
      "Epoch: 2980, G_Loss: 3.6909, D_Loss: 0.0378\n",
      "Epoch: 2990, G_Loss: 3.7149, D_Loss: 0.0393\n",
      "Epoch: 3000, G_Loss: 3.6648, D_Loss: 0.0385\n",
      "Epoch: 3010, G_Loss: 3.7468, D_Loss: 0.0377\n",
      "Epoch: 3020, G_Loss: 3.7443, D_Loss: 0.0361\n",
      "Epoch: 3030, G_Loss: 3.7308, D_Loss: 0.0378\n",
      "Epoch: 3040, G_Loss: 3.8034, D_Loss: 0.0386\n",
      "Epoch: 3050, G_Loss: 3.7618, D_Loss: 0.0369\n",
      "Epoch: 3060, G_Loss: 3.7897, D_Loss: 0.0380\n",
      "Epoch: 3070, G_Loss: 3.8222, D_Loss: 0.0386\n",
      "Epoch: 3080, G_Loss: 3.7991, D_Loss: 0.0336\n",
      "Epoch: 3090, G_Loss: 3.8106, D_Loss: 0.0324\n",
      "Epoch: 3100, G_Loss: 3.7914, D_Loss: 0.0362\n",
      "Epoch: 3110, G_Loss: 3.8132, D_Loss: 0.0348\n",
      "Epoch: 3120, G_Loss: 3.7078, D_Loss: 0.0388\n",
      "Epoch: 3130, G_Loss: 3.7301, D_Loss: 0.0423\n",
      "Epoch: 3140, G_Loss: 3.7571, D_Loss: 0.0389\n",
      "Epoch: 3150, G_Loss: 3.6998, D_Loss: 0.0337\n",
      "Epoch: 3160, G_Loss: 3.5872, D_Loss: 0.0383\n",
      "Epoch: 3170, G_Loss: 3.5844, D_Loss: 0.0401\n",
      "Epoch: 3180, G_Loss: 3.6975, D_Loss: 0.0428\n",
      "Epoch: 3190, G_Loss: 3.7303, D_Loss: 0.0408\n",
      "Epoch: 3200, G_Loss: 3.7682, D_Loss: 0.0346\n",
      "Epoch: 3210, G_Loss: 3.8299, D_Loss: 0.0313\n",
      "Epoch: 3220, G_Loss: 3.8342, D_Loss: 0.0285\n",
      "Epoch: 3230, G_Loss: 3.9237, D_Loss: 0.0307\n",
      "Epoch: 3240, G_Loss: 4.0449, D_Loss: 0.0282\n",
      "Epoch: 3250, G_Loss: 4.0553, D_Loss: 0.0250\n",
      "Epoch: 3260, G_Loss: 4.0485, D_Loss: 0.0252\n",
      "Epoch: 3270, G_Loss: 4.0454, D_Loss: 0.0246\n",
      "Epoch: 3280, G_Loss: 4.0915, D_Loss: 0.0219\n",
      "Epoch: 3290, G_Loss: 4.0818, D_Loss: 0.0240\n",
      "Epoch: 3300, G_Loss: 4.0875, D_Loss: 0.0246\n",
      "Epoch: 3310, G_Loss: 4.0711, D_Loss: 0.0256\n",
      "Epoch: 3320, G_Loss: 4.1464, D_Loss: 0.0278\n",
      "Epoch: 3330, G_Loss: 4.0035, D_Loss: 0.0259\n",
      "Epoch: 3340, G_Loss: 4.0372, D_Loss: 0.0276\n",
      "Epoch: 3350, G_Loss: 4.0890, D_Loss: 0.0283\n",
      "Epoch: 3360, G_Loss: 4.1019, D_Loss: 0.0290\n",
      "Epoch: 3370, G_Loss: 4.0008, D_Loss: 0.0331\n",
      "Epoch: 3380, G_Loss: 4.0810, D_Loss: 0.0273\n",
      "Epoch: 3390, G_Loss: 4.0349, D_Loss: 0.0269\n",
      "Epoch: 3400, G_Loss: 4.0152, D_Loss: 0.0283\n",
      "Epoch: 3410, G_Loss: 4.0454, D_Loss: 0.0240\n",
      "Epoch: 3420, G_Loss: 4.1022, D_Loss: 0.0245\n",
      "Epoch: 3430, G_Loss: 4.1668, D_Loss: 0.0248\n",
      "Epoch: 3440, G_Loss: 4.0275, D_Loss: 0.0275\n",
      "Epoch: 3450, G_Loss: 4.1248, D_Loss: 0.0236\n",
      "Epoch: 3460, G_Loss: 4.1403, D_Loss: 0.0289\n",
      "Epoch: 3470, G_Loss: 4.1586, D_Loss: 0.0271\n",
      "Epoch: 3480, G_Loss: 4.2167, D_Loss: 0.0232\n",
      "Epoch: 3490, G_Loss: 4.2033, D_Loss: 0.0238\n",
      "Epoch: 3500, G_Loss: 4.2719, D_Loss: 0.0241\n",
      "Epoch: 3510, G_Loss: 4.2308, D_Loss: 0.0239\n",
      "Epoch: 3520, G_Loss: 4.2974, D_Loss: 0.0221\n",
      "Epoch: 3530, G_Loss: 4.3049, D_Loss: 0.0220\n",
      "Epoch: 3540, G_Loss: 4.3665, D_Loss: 0.0196\n",
      "Epoch: 3550, G_Loss: 4.3730, D_Loss: 0.0200\n",
      "Epoch: 3560, G_Loss: 4.4798, D_Loss: 0.0197\n",
      "Epoch: 3570, G_Loss: 4.4119, D_Loss: 0.0188\n",
      "Epoch: 3580, G_Loss: 4.4902, D_Loss: 0.0178\n",
      "Epoch: 3590, G_Loss: 4.5176, D_Loss: 0.0176\n",
      "Epoch: 3600, G_Loss: 4.4491, D_Loss: 0.0163\n",
      "Epoch: 3610, G_Loss: 4.4048, D_Loss: 0.0191\n",
      "Epoch: 3620, G_Loss: 4.4599, D_Loss: 0.0201\n",
      "Epoch: 3630, G_Loss: 4.4546, D_Loss: 0.0193\n",
      "Epoch: 3640, G_Loss: 4.4928, D_Loss: 0.0186\n",
      "Epoch: 3650, G_Loss: 4.4544, D_Loss: 0.0182\n",
      "Epoch: 3660, G_Loss: 4.4830, D_Loss: 0.0180\n",
      "Epoch: 3670, G_Loss: 4.3590, D_Loss: 0.0221\n",
      "Epoch: 3680, G_Loss: 4.3791, D_Loss: 0.0201\n",
      "Epoch: 3690, G_Loss: 4.3782, D_Loss: 0.0214\n",
      "Epoch: 3700, G_Loss: 4.4133, D_Loss: 0.0208\n",
      "Epoch: 3710, G_Loss: 4.3776, D_Loss: 0.0200\n",
      "Epoch: 3720, G_Loss: 4.4419, D_Loss: 0.0191\n",
      "Epoch: 3730, G_Loss: 4.4807, D_Loss: 0.0183\n",
      "Epoch: 3740, G_Loss: 4.4532, D_Loss: 0.0175\n",
      "Epoch: 3750, G_Loss: 4.4831, D_Loss: 0.0173\n",
      "Epoch: 3760, G_Loss: 4.4593, D_Loss: 0.0169\n",
      "Epoch: 3770, G_Loss: 4.4601, D_Loss: 0.0163\n",
      "Epoch: 3780, G_Loss: 4.5341, D_Loss: 0.0164\n",
      "Epoch: 3790, G_Loss: 4.5900, D_Loss: 0.0179\n",
      "Epoch: 3800, G_Loss: 4.5821, D_Loss: 0.0160\n",
      "Epoch: 3810, G_Loss: 4.5920, D_Loss: 0.0169\n",
      "Epoch: 3820, G_Loss: 4.5410, D_Loss: 0.0170\n",
      "Epoch: 3830, G_Loss: 4.6429, D_Loss: 0.0162\n",
      "Epoch: 3840, G_Loss: 4.6789, D_Loss: 0.0157\n",
      "Epoch: 3850, G_Loss: 4.6111, D_Loss: 0.0176\n",
      "Epoch: 3860, G_Loss: 4.5905, D_Loss: 0.0152\n",
      "Epoch: 3870, G_Loss: 4.6079, D_Loss: 0.0167\n",
      "Epoch: 3880, G_Loss: 4.6349, D_Loss: 0.0144\n",
      "Epoch: 3890, G_Loss: 4.6385, D_Loss: 0.0144\n",
      "Epoch: 3900, G_Loss: 4.5993, D_Loss: 0.0150\n",
      "Epoch: 3910, G_Loss: 4.6428, D_Loss: 0.0150\n",
      "Epoch: 3920, G_Loss: 4.6931, D_Loss: 0.0136\n",
      "Epoch: 3930, G_Loss: 4.7588, D_Loss: 0.0131\n",
      "Epoch: 3940, G_Loss: 4.6944, D_Loss: 0.0137\n",
      "Epoch: 3950, G_Loss: 4.7335, D_Loss: 0.0133\n",
      "Epoch: 3960, G_Loss: 4.6886, D_Loss: 0.0146\n",
      "Epoch: 3970, G_Loss: 4.7949, D_Loss: 0.0149\n",
      "Epoch: 3980, G_Loss: 4.7739, D_Loss: 0.0145\n",
      "Epoch: 3990, G_Loss: 4.8230, D_Loss: 0.0143\n",
      "Epoch: 4000, G_Loss: 4.7729, D_Loss: 0.0129\n",
      "Epoch: 4010, G_Loss: 4.8410, D_Loss: 0.0140\n",
      "Epoch: 4020, G_Loss: 4.8550, D_Loss: 0.0124\n",
      "Epoch: 4030, G_Loss: 4.8946, D_Loss: 0.0125\n",
      "Epoch: 4040, G_Loss: 4.8528, D_Loss: 0.0116\n",
      "Epoch: 4050, G_Loss: 4.8057, D_Loss: 0.0141\n",
      "Epoch: 4060, G_Loss: 4.8110, D_Loss: 0.0122\n",
      "Epoch: 4070, G_Loss: 4.7984, D_Loss: 0.0144\n",
      "Epoch: 4080, G_Loss: 4.8137, D_Loss: 0.0123\n",
      "Epoch: 4090, G_Loss: 4.8680, D_Loss: 0.0133\n",
      "Epoch: 4100, G_Loss: 4.8540, D_Loss: 0.0126\n",
      "Epoch: 4110, G_Loss: 4.9236, D_Loss: 0.0113\n",
      "Epoch: 4120, G_Loss: 4.8814, D_Loss: 0.0131\n",
      "Epoch: 4130, G_Loss: 4.9601, D_Loss: 0.0123\n",
      "Epoch: 4140, G_Loss: 4.9629, D_Loss: 0.0110\n",
      "Epoch: 4150, G_Loss: 5.0410, D_Loss: 0.0104\n",
      "Epoch: 4160, G_Loss: 5.0701, D_Loss: 0.0115\n",
      "Epoch: 4170, G_Loss: 5.0796, D_Loss: 0.0092\n",
      "Epoch: 4180, G_Loss: 5.1430, D_Loss: 0.0088\n",
      "Epoch: 4190, G_Loss: 5.1680, D_Loss: 0.0095\n",
      "Epoch: 4200, G_Loss: 5.1538, D_Loss: 0.0093\n",
      "Epoch: 4210, G_Loss: 5.0970, D_Loss: 0.0087\n",
      "Epoch: 4220, G_Loss: 5.1458, D_Loss: 0.0091\n",
      "Epoch: 4230, G_Loss: 5.0801, D_Loss: 0.0092\n",
      "Epoch: 4240, G_Loss: 5.1058, D_Loss: 0.0096\n",
      "Epoch: 4250, G_Loss: 5.0774, D_Loss: 0.0100\n",
      "Epoch: 4260, G_Loss: 5.0160, D_Loss: 0.0096\n",
      "Epoch: 4270, G_Loss: 5.0951, D_Loss: 0.0105\n",
      "Epoch: 4280, G_Loss: 5.2077, D_Loss: 0.0096\n",
      "Epoch: 4290, G_Loss: 5.1234, D_Loss: 0.0094\n",
      "Epoch: 4300, G_Loss: 5.0834, D_Loss: 0.0093\n",
      "Epoch: 4310, G_Loss: 5.0834, D_Loss: 0.0099\n",
      "Epoch: 4320, G_Loss: 5.1721, D_Loss: 0.0091\n",
      "Epoch: 4330, G_Loss: 5.1377, D_Loss: 0.0091\n",
      "Epoch: 4340, G_Loss: 5.1574, D_Loss: 0.0089\n",
      "Epoch: 4350, G_Loss: 5.0693, D_Loss: 0.0104\n",
      "Epoch: 4360, G_Loss: 5.1229, D_Loss: 0.0099\n",
      "Epoch: 4370, G_Loss: 5.0914, D_Loss: 0.0092\n",
      "Epoch: 4380, G_Loss: 5.1331, D_Loss: 0.0102\n",
      "Epoch: 4390, G_Loss: 5.1623, D_Loss: 0.0082\n",
      "Epoch: 4400, G_Loss: 5.2450, D_Loss: 0.0099\n",
      "Epoch: 4410, G_Loss: 5.1947, D_Loss: 0.0080\n",
      "Epoch: 4420, G_Loss: 5.2410, D_Loss: 0.0089\n",
      "Epoch: 4430, G_Loss: 5.3241, D_Loss: 0.0078\n",
      "Epoch: 4440, G_Loss: 5.4165, D_Loss: 0.0072\n",
      "Epoch: 4450, G_Loss: 5.4215, D_Loss: 0.0073\n",
      "Epoch: 4460, G_Loss: 5.3914, D_Loss: 0.0076\n",
      "Epoch: 4470, G_Loss: 5.4969, D_Loss: 0.0078\n",
      "Epoch: 4480, G_Loss: 5.4096, D_Loss: 0.0070\n",
      "Epoch: 4490, G_Loss: 5.4463, D_Loss: 0.0073\n",
      "Epoch: 4500, G_Loss: 5.5445, D_Loss: 0.0067\n",
      "Epoch: 4510, G_Loss: 5.5243, D_Loss: 0.0059\n",
      "Epoch: 4520, G_Loss: 5.5939, D_Loss: 0.0064\n",
      "Epoch: 4530, G_Loss: 5.4801, D_Loss: 0.0056\n",
      "Epoch: 4540, G_Loss: 5.5409, D_Loss: 0.0062\n",
      "Epoch: 4550, G_Loss: 5.4835, D_Loss: 0.0064\n",
      "Epoch: 4560, G_Loss: 5.5663, D_Loss: 0.0069\n",
      "Epoch: 4570, G_Loss: 5.4780, D_Loss: 0.0071\n",
      "Epoch: 4580, G_Loss: 5.6108, D_Loss: 0.0060\n",
      "Epoch: 4590, G_Loss: 5.5042, D_Loss: 0.0068\n",
      "Epoch: 4600, G_Loss: 5.5017, D_Loss: 0.0062\n",
      "Epoch: 4610, G_Loss: 5.4621, D_Loss: 0.0061\n",
      "Epoch: 4620, G_Loss: 5.5370, D_Loss: 0.0065\n",
      "Epoch: 4630, G_Loss: 5.4990, D_Loss: 0.0070\n",
      "Epoch: 4640, G_Loss: 5.4806, D_Loss: 0.0069\n",
      "Epoch: 4650, G_Loss: 5.5542, D_Loss: 0.0059\n",
      "Epoch: 4660, G_Loss: 5.5897, D_Loss: 0.0059\n",
      "Epoch: 4670, G_Loss: 5.5410, D_Loss: 0.0063\n",
      "Epoch: 4680, G_Loss: 5.5438, D_Loss: 0.0069\n",
      "Epoch: 4690, G_Loss: 5.4497, D_Loss: 0.0065\n",
      "Epoch: 4700, G_Loss: 5.5800, D_Loss: 0.0071\n",
      "Epoch: 4710, G_Loss: 5.4729, D_Loss: 0.0068\n",
      "Epoch: 4720, G_Loss: 5.4437, D_Loss: 0.0072\n",
      "Epoch: 4730, G_Loss: 5.5018, D_Loss: 0.0071\n",
      "Epoch: 4740, G_Loss: 5.4844, D_Loss: 0.0057\n",
      "Epoch: 4750, G_Loss: 5.5336, D_Loss: 0.0077\n",
      "Epoch: 4760, G_Loss: 5.5832, D_Loss: 0.0071\n",
      "Epoch: 4770, G_Loss: 5.5924, D_Loss: 0.0064\n",
      "Epoch: 4780, G_Loss: 5.5876, D_Loss: 0.0058\n",
      "Epoch: 4790, G_Loss: 5.6578, D_Loss: 0.0054\n",
      "Epoch: 4800, G_Loss: 5.6074, D_Loss: 0.0052\n",
      "Epoch: 4810, G_Loss: 5.6226, D_Loss: 0.0056\n",
      "Epoch: 4820, G_Loss: 5.7388, D_Loss: 0.0051\n",
      "Epoch: 4830, G_Loss: 5.7738, D_Loss: 0.0049\n",
      "Epoch: 4840, G_Loss: 5.7729, D_Loss: 0.0049\n",
      "Epoch: 4850, G_Loss: 5.7793, D_Loss: 0.0055\n",
      "Epoch: 4860, G_Loss: 5.6819, D_Loss: 0.0053\n",
      "Epoch: 4870, G_Loss: 5.6992, D_Loss: 0.0051\n",
      "Epoch: 4880, G_Loss: 5.7085, D_Loss: 0.0057\n",
      "Epoch: 4890, G_Loss: 5.7462, D_Loss: 0.0046\n",
      "Epoch: 4900, G_Loss: 5.7451, D_Loss: 0.0049\n",
      "Epoch: 4910, G_Loss: 5.7274, D_Loss: 0.0060\n",
      "Epoch: 4920, G_Loss: 5.7372, D_Loss: 0.0050\n",
      "Epoch: 4930, G_Loss: 5.7182, D_Loss: 0.0057\n",
      "Epoch: 4940, G_Loss: 5.6905, D_Loss: 0.0050\n",
      "Epoch: 4950, G_Loss: 5.7540, D_Loss: 0.0051\n",
      "Epoch: 4960, G_Loss: 5.8049, D_Loss: 0.0051\n",
      "Epoch: 4970, G_Loss: 5.8555, D_Loss: 0.0052\n",
      "Epoch: 4980, G_Loss: 5.8522, D_Loss: 0.0043\n",
      "Epoch: 4990, G_Loss: 5.8494, D_Loss: 0.0045\n",
      "Epoch: 5000, G_Loss: 5.8492, D_Loss: 0.0046\n",
      "Epoch: 5010, G_Loss: 5.8306, D_Loss: 0.0043\n",
      "Epoch: 5020, G_Loss: 5.7776, D_Loss: 0.0050\n",
      "Epoch: 5030, G_Loss: 5.8024, D_Loss: 0.0048\n",
      "Epoch: 5040, G_Loss: 5.7792, D_Loss: 0.0048\n",
      "Epoch: 5050, G_Loss: 5.9012, D_Loss: 0.0047\n",
      "Epoch: 5060, G_Loss: 5.8585, D_Loss: 0.0048\n",
      "Epoch: 5070, G_Loss: 5.7987, D_Loss: 0.0056\n",
      "Epoch: 5080, G_Loss: 5.8230, D_Loss: 0.0052\n",
      "Epoch: 5090, G_Loss: 5.8204, D_Loss: 0.0049\n",
      "Epoch: 5100, G_Loss: 5.8362, D_Loss: 0.0041\n",
      "Epoch: 5110, G_Loss: 5.8364, D_Loss: 0.0043\n",
      "Epoch: 5120, G_Loss: 5.8313, D_Loss: 0.0050\n",
      "Epoch: 5130, G_Loss: 5.8626, D_Loss: 0.0051\n",
      "Epoch: 5140, G_Loss: 5.8658, D_Loss: 0.0051\n",
      "Epoch: 5150, G_Loss: 5.8324, D_Loss: 0.0045\n",
      "Epoch: 5160, G_Loss: 5.9471, D_Loss: 0.0047\n",
      "Epoch: 5170, G_Loss: 5.8111, D_Loss: 0.0049\n",
      "Epoch: 5180, G_Loss: 5.9245, D_Loss: 0.0039\n",
      "Epoch: 5190, G_Loss: 5.9307, D_Loss: 0.0044\n",
      "Epoch: 5200, G_Loss: 5.9176, D_Loss: 0.0044\n",
      "Epoch: 5210, G_Loss: 5.9422, D_Loss: 0.0040\n",
      "Epoch: 5220, G_Loss: 5.9416, D_Loss: 0.0043\n",
      "Epoch: 5230, G_Loss: 5.9900, D_Loss: 0.0043\n",
      "Epoch: 5240, G_Loss: 5.9382, D_Loss: 0.0044\n",
      "Epoch: 5250, G_Loss: 5.9753, D_Loss: 0.0044\n",
      "Epoch: 5260, G_Loss: 5.9722, D_Loss: 0.0048\n",
      "Epoch: 5270, G_Loss: 6.0080, D_Loss: 0.0038\n",
      "Epoch: 5280, G_Loss: 5.9040, D_Loss: 0.0045\n",
      "Epoch: 5290, G_Loss: 6.0883, D_Loss: 0.0045\n",
      "Epoch: 5300, G_Loss: 5.9939, D_Loss: 0.0041\n",
      "Epoch: 5310, G_Loss: 6.0241, D_Loss: 0.0039\n",
      "Epoch: 5320, G_Loss: 6.1183, D_Loss: 0.0040\n",
      "Epoch: 5330, G_Loss: 6.1028, D_Loss: 0.0036\n",
      "Epoch: 5340, G_Loss: 6.2522, D_Loss: 0.0038\n",
      "Epoch: 5350, G_Loss: 6.3203, D_Loss: 0.0036\n",
      "Epoch: 5360, G_Loss: 6.2883, D_Loss: 0.0032\n",
      "Epoch: 5370, G_Loss: 6.3909, D_Loss: 0.0027\n",
      "Epoch: 5380, G_Loss: 6.3387, D_Loss: 0.0031\n",
      "Epoch: 5390, G_Loss: 6.2632, D_Loss: 0.0034\n",
      "Epoch: 5400, G_Loss: 6.2953, D_Loss: 0.0034\n",
      "Epoch: 5410, G_Loss: 6.3707, D_Loss: 0.0034\n",
      "Epoch: 5420, G_Loss: 6.2836, D_Loss: 0.0032\n",
      "Epoch: 5430, G_Loss: 6.2701, D_Loss: 0.0032\n",
      "Epoch: 5440, G_Loss: 6.2774, D_Loss: 0.0030\n",
      "Epoch: 5450, G_Loss: 6.2150, D_Loss: 0.0032\n",
      "Epoch: 5460, G_Loss: 6.2451, D_Loss: 0.0036\n",
      "Epoch: 5470, G_Loss: 6.2987, D_Loss: 0.0033\n",
      "Epoch: 5480, G_Loss: 6.3629, D_Loss: 0.0030\n",
      "Epoch: 5490, G_Loss: 6.3141, D_Loss: 0.0035\n",
      "Epoch: 5500, G_Loss: 6.3054, D_Loss: 0.0035\n",
      "Epoch: 5510, G_Loss: 6.2770, D_Loss: 0.0030\n",
      "Epoch: 5520, G_Loss: 6.3196, D_Loss: 0.0029\n",
      "Epoch: 5530, G_Loss: 6.3395, D_Loss: 0.0032\n",
      "Epoch: 5540, G_Loss: 6.3591, D_Loss: 0.0031\n",
      "Epoch: 5550, G_Loss: 6.2887, D_Loss: 0.0030\n",
      "Epoch: 5560, G_Loss: 6.2561, D_Loss: 0.0030\n",
      "Epoch: 5570, G_Loss: 6.2865, D_Loss: 0.0028\n",
      "Epoch: 5580, G_Loss: 6.2296, D_Loss: 0.0032\n",
      "Epoch: 5590, G_Loss: 6.3685, D_Loss: 0.0032\n",
      "Epoch: 5600, G_Loss: 6.4211, D_Loss: 0.0026\n",
      "Epoch: 5610, G_Loss: 6.4413, D_Loss: 0.0026\n",
      "Epoch: 5620, G_Loss: 6.4822, D_Loss: 0.0028\n",
      "Epoch: 5630, G_Loss: 6.5389, D_Loss: 0.0022\n",
      "Epoch: 5640, G_Loss: 6.4345, D_Loss: 0.0028\n",
      "Epoch: 5650, G_Loss: 6.5820, D_Loss: 0.0026\n",
      "Epoch: 5660, G_Loss: 6.5497, D_Loss: 0.0027\n",
      "Epoch: 5670, G_Loss: 6.5080, D_Loss: 0.0023\n",
      "Epoch: 5680, G_Loss: 6.4696, D_Loss: 0.0024\n",
      "Epoch: 5690, G_Loss: 6.6001, D_Loss: 0.0024\n",
      "Epoch: 5700, G_Loss: 6.5567, D_Loss: 0.0025\n",
      "Epoch: 5710, G_Loss: 6.5315, D_Loss: 0.0022\n",
      "Epoch: 5720, G_Loss: 6.5969, D_Loss: 0.0023\n",
      "Epoch: 5730, G_Loss: 6.5072, D_Loss: 0.0025\n",
      "Epoch: 5740, G_Loss: 6.5943, D_Loss: 0.0027\n",
      "Epoch: 5750, G_Loss: 6.5631, D_Loss: 0.0027\n",
      "Epoch: 5760, G_Loss: 6.6048, D_Loss: 0.0024\n",
      "Epoch: 5770, G_Loss: 6.6130, D_Loss: 0.0021\n",
      "Epoch: 5780, G_Loss: 6.6592, D_Loss: 0.0022\n",
      "Epoch: 5790, G_Loss: 6.7014, D_Loss: 0.0024\n",
      "Epoch: 5800, G_Loss: 6.6230, D_Loss: 0.0021\n",
      "Epoch: 5810, G_Loss: 6.6203, D_Loss: 0.0018\n",
      "Epoch: 5820, G_Loss: 6.7283, D_Loss: 0.0022\n",
      "Epoch: 5830, G_Loss: 6.7111, D_Loss: 0.0021\n",
      "Epoch: 5840, G_Loss: 6.7337, D_Loss: 0.0018\n",
      "Epoch: 5850, G_Loss: 6.8573, D_Loss: 0.0015\n",
      "Epoch: 5860, G_Loss: 6.8699, D_Loss: 0.0019\n",
      "Epoch: 5870, G_Loss: 6.8131, D_Loss: 0.0017\n",
      "Epoch: 5880, G_Loss: 6.8255, D_Loss: 0.0017\n",
      "Epoch: 5890, G_Loss: 6.7391, D_Loss: 0.0017\n",
      "Epoch: 5900, G_Loss: 6.7355, D_Loss: 0.0018\n",
      "Epoch: 5910, G_Loss: 6.7102, D_Loss: 0.0019\n",
      "Epoch: 5920, G_Loss: 6.8348, D_Loss: 0.0018\n",
      "Epoch: 5930, G_Loss: 6.8023, D_Loss: 0.0016\n",
      "Epoch: 5940, G_Loss: 6.8497, D_Loss: 0.0017\n",
      "Epoch: 5950, G_Loss: 6.8150, D_Loss: 0.0018\n",
      "Epoch: 5960, G_Loss: 6.9013, D_Loss: 0.0017\n",
      "Epoch: 5970, G_Loss: 6.7861, D_Loss: 0.0019\n",
      "Epoch: 5980, G_Loss: 6.8845, D_Loss: 0.0018\n",
      "Epoch: 5990, G_Loss: 6.7926, D_Loss: 0.0020\n",
      "Epoch: 6000, G_Loss: 6.8143, D_Loss: 0.0020\n",
      "Epoch: 6010, G_Loss: 6.8286, D_Loss: 0.0019\n",
      "Epoch: 6020, G_Loss: 6.7413, D_Loss: 0.0021\n",
      "Epoch: 6030, G_Loss: 6.8015, D_Loss: 0.0019\n",
      "Epoch: 6040, G_Loss: 6.6444, D_Loss: 0.0020\n",
      "Epoch: 6050, G_Loss: 6.8503, D_Loss: 0.0019\n",
      "Epoch: 6060, G_Loss: 6.9716, D_Loss: 0.0018\n",
      "Epoch: 6070, G_Loss: 6.8664, D_Loss: 0.0017\n",
      "Epoch: 6080, G_Loss: 6.8106, D_Loss: 0.0017\n",
      "Epoch: 6090, G_Loss: 6.8476, D_Loss: 0.0017\n",
      "Epoch: 6100, G_Loss: 6.8889, D_Loss: 0.0017\n",
      "Epoch: 6110, G_Loss: 6.8642, D_Loss: 0.0017\n",
      "Epoch: 6120, G_Loss: 6.8254, D_Loss: 0.0017\n",
      "Epoch: 6130, G_Loss: 6.8662, D_Loss: 0.0016\n",
      "Epoch: 6140, G_Loss: 6.9775, D_Loss: 0.0017\n",
      "Epoch: 6150, G_Loss: 6.9425, D_Loss: 0.0015\n",
      "Epoch: 6160, G_Loss: 7.0366, D_Loss: 0.0015\n",
      "Epoch: 6170, G_Loss: 7.0688, D_Loss: 0.0016\n",
      "Epoch: 6180, G_Loss: 7.0394, D_Loss: 0.0015\n",
      "Epoch: 6190, G_Loss: 7.1370, D_Loss: 0.0014\n",
      "Epoch: 6200, G_Loss: 7.0468, D_Loss: 0.0013\n",
      "Epoch: 6210, G_Loss: 7.1499, D_Loss: 0.0012\n",
      "Epoch: 6220, G_Loss: 7.1886, D_Loss: 0.0012\n",
      "Epoch: 6230, G_Loss: 7.1567, D_Loss: 0.0011\n",
      "Epoch: 6240, G_Loss: 7.1784, D_Loss: 0.0010\n",
      "Epoch: 6250, G_Loss: 7.2399, D_Loss: 0.0013\n",
      "Epoch: 6260, G_Loss: 7.2382, D_Loss: 0.0013\n",
      "Epoch: 6270, G_Loss: 7.1492, D_Loss: 0.0013\n",
      "Epoch: 6280, G_Loss: 7.1537, D_Loss: 0.0011\n",
      "Epoch: 6290, G_Loss: 7.1904, D_Loss: 0.0013\n",
      "Epoch: 6300, G_Loss: 7.3338, D_Loss: 0.0014\n",
      "Epoch: 6310, G_Loss: 7.2306, D_Loss: 0.0011\n",
      "Epoch: 6320, G_Loss: 7.1841, D_Loss: 0.0013\n",
      "Epoch: 6330, G_Loss: 7.2943, D_Loss: 0.0012\n",
      "Epoch: 6340, G_Loss: 7.3543, D_Loss: 0.0010\n",
      "Epoch: 6350, G_Loss: 7.4308, D_Loss: 0.0009\n",
      "Epoch: 6360, G_Loss: 7.3779, D_Loss: 0.0011\n",
      "Epoch: 6370, G_Loss: 7.4776, D_Loss: 0.0009\n",
      "Epoch: 6380, G_Loss: 7.3701, D_Loss: 0.0011\n",
      "Epoch: 6390, G_Loss: 7.4077, D_Loss: 0.0009\n",
      "Epoch: 6400, G_Loss: 7.3375, D_Loss: 0.0009\n",
      "Epoch: 6410, G_Loss: 7.3695, D_Loss: 0.0008\n",
      "Epoch: 6420, G_Loss: 7.4576, D_Loss: 0.0010\n",
      "Epoch: 6430, G_Loss: 7.4498, D_Loss: 0.0010\n",
      "Epoch: 6440, G_Loss: 7.3178, D_Loss: 0.0012\n",
      "Epoch: 6450, G_Loss: 7.5043, D_Loss: 0.0011\n",
      "Epoch: 6460, G_Loss: 7.3560, D_Loss: 0.0011\n",
      "Epoch: 6470, G_Loss: 7.4294, D_Loss: 0.0011\n",
      "Epoch: 6480, G_Loss: 7.5026, D_Loss: 0.0009\n",
      "Epoch: 6490, G_Loss: 7.4925, D_Loss: 0.0009\n",
      "Epoch: 6500, G_Loss: 7.4691, D_Loss: 0.0010\n",
      "Epoch: 6510, G_Loss: 7.4759, D_Loss: 0.0010\n",
      "Epoch: 6520, G_Loss: 7.5491, D_Loss: 0.0009\n",
      "Epoch: 6530, G_Loss: 7.5550, D_Loss: 0.0009\n",
      "Epoch: 6540, G_Loss: 7.5296, D_Loss: 0.0010\n",
      "Epoch: 6550, G_Loss: 7.4660, D_Loss: 0.0009\n",
      "Epoch: 6560, G_Loss: 7.4510, D_Loss: 0.0010\n",
      "Epoch: 6570, G_Loss: 7.5104, D_Loss: 0.0009\n",
      "Epoch: 6580, G_Loss: 7.5036, D_Loss: 0.0008\n",
      "Epoch: 6590, G_Loss: 7.5393, D_Loss: 0.0009\n",
      "Epoch: 6600, G_Loss: 7.5664, D_Loss: 0.0007\n",
      "Epoch: 6610, G_Loss: 7.6751, D_Loss: 0.0008\n",
      "Epoch: 6620, G_Loss: 7.6719, D_Loss: 0.0009\n",
      "Epoch: 6630, G_Loss: 7.7288, D_Loss: 0.0008\n",
      "Epoch: 6640, G_Loss: 7.7578, D_Loss: 0.0008\n",
      "Epoch: 6650, G_Loss: 7.7421, D_Loss: 0.0007\n",
      "Epoch: 6660, G_Loss: 7.7824, D_Loss: 0.0007\n",
      "Epoch: 6670, G_Loss: 7.8417, D_Loss: 0.0006\n",
      "Epoch: 6680, G_Loss: 7.8871, D_Loss: 0.0007\n",
      "Epoch: 6690, G_Loss: 7.9128, D_Loss: 0.0007\n",
      "Epoch: 6700, G_Loss: 7.8950, D_Loss: 0.0006\n",
      "Epoch: 6710, G_Loss: 7.9362, D_Loss: 0.0006\n",
      "Epoch: 6720, G_Loss: 7.9179, D_Loss: 0.0005\n",
      "Epoch: 6730, G_Loss: 7.9124, D_Loss: 0.0006\n",
      "Epoch: 6740, G_Loss: 7.9217, D_Loss: 0.0006\n",
      "Epoch: 6750, G_Loss: 7.8634, D_Loss: 0.0007\n",
      "Epoch: 6760, G_Loss: 7.8264, D_Loss: 0.0007\n",
      "Epoch: 6770, G_Loss: 7.7922, D_Loss: 0.0008\n",
      "Epoch: 6780, G_Loss: 7.7806, D_Loss: 0.0006\n",
      "Epoch: 6790, G_Loss: 7.7862, D_Loss: 0.0006\n",
      "Epoch: 6800, G_Loss: 7.8847, D_Loss: 0.0006\n",
      "Epoch: 6810, G_Loss: 7.7892, D_Loss: 0.0008\n",
      "Epoch: 6820, G_Loss: 7.7656, D_Loss: 0.0007\n",
      "Epoch: 6830, G_Loss: 7.8388, D_Loss: 0.0008\n",
      "Epoch: 6840, G_Loss: 7.8339, D_Loss: 0.0006\n",
      "Epoch: 6850, G_Loss: 7.7251, D_Loss: 0.0007\n",
      "Epoch: 6860, G_Loss: 7.7079, D_Loss: 0.0006\n",
      "Epoch: 6870, G_Loss: 7.8442, D_Loss: 0.0007\n",
      "Epoch: 6880, G_Loss: 7.8588, D_Loss: 0.0007\n",
      "Epoch: 6890, G_Loss: 7.9094, D_Loss: 0.0006\n",
      "Epoch: 6900, G_Loss: 7.9387, D_Loss: 0.0007\n",
      "Epoch: 6910, G_Loss: 7.9775, D_Loss: 0.0006\n",
      "Epoch: 6920, G_Loss: 7.9574, D_Loss: 0.0005\n",
      "Epoch: 6930, G_Loss: 7.9917, D_Loss: 0.0006\n",
      "Epoch: 6940, G_Loss: 8.0391, D_Loss: 0.0006\n",
      "Epoch: 6950, G_Loss: 8.1249, D_Loss: 0.0005\n",
      "Epoch: 6960, G_Loss: 8.0612, D_Loss: 0.0005\n",
      "Epoch: 6970, G_Loss: 8.1183, D_Loss: 0.0005\n",
      "Epoch: 6980, G_Loss: 8.0830, D_Loss: 0.0005\n",
      "Epoch: 6990, G_Loss: 8.1705, D_Loss: 0.0005\n",
      "Epoch: 7000, G_Loss: 8.0260, D_Loss: 0.0005\n",
      "Epoch: 7010, G_Loss: 8.1491, D_Loss: 0.0004\n",
      "Epoch: 7020, G_Loss: 8.1896, D_Loss: 0.0004\n",
      "Epoch: 7030, G_Loss: 8.2634, D_Loss: 0.0005\n",
      "Epoch: 7040, G_Loss: 8.2703, D_Loss: 0.0005\n",
      "Epoch: 7050, G_Loss: 8.2787, D_Loss: 0.0004\n",
      "Epoch: 7060, G_Loss: 8.3240, D_Loss: 0.0004\n",
      "Epoch: 7070, G_Loss: 8.3541, D_Loss: 0.0004\n",
      "Epoch: 7080, G_Loss: 8.3778, D_Loss: 0.0003\n",
      "Epoch: 7090, G_Loss: 8.4268, D_Loss: 0.0003\n",
      "Epoch: 7100, G_Loss: 8.4353, D_Loss: 0.0004\n",
      "Epoch: 7110, G_Loss: 8.3605, D_Loss: 0.0004\n",
      "Epoch: 7120, G_Loss: 8.3157, D_Loss: 0.0004\n",
      "Epoch: 7130, G_Loss: 8.3485, D_Loss: 0.0004\n",
      "Epoch: 7140, G_Loss: 8.2794, D_Loss: 0.0004\n",
      "Epoch: 7150, G_Loss: 8.3644, D_Loss: 0.0004\n",
      "Epoch: 7160, G_Loss: 8.3056, D_Loss: 0.0004\n",
      "Epoch: 7170, G_Loss: 8.3140, D_Loss: 0.0004\n",
      "Epoch: 7180, G_Loss: 8.4010, D_Loss: 0.0003\n",
      "Epoch: 7190, G_Loss: 8.3135, D_Loss: 0.0004\n",
      "Epoch: 7200, G_Loss: 8.3394, D_Loss: 0.0003\n",
      "Epoch: 7210, G_Loss: 8.3792, D_Loss: 0.0004\n",
      "Epoch: 7220, G_Loss: 8.3541, D_Loss: 0.0003\n",
      "Epoch: 7230, G_Loss: 8.2868, D_Loss: 0.0003\n",
      "Epoch: 7240, G_Loss: 8.2976, D_Loss: 0.0003\n",
      "Epoch: 7250, G_Loss: 8.3467, D_Loss: 0.0003\n",
      "Epoch: 7260, G_Loss: 8.4915, D_Loss: 0.0003\n",
      "Epoch: 7270, G_Loss: 8.4182, D_Loss: 0.0003\n",
      "Epoch: 7280, G_Loss: 8.3432, D_Loss: 0.0004\n",
      "Epoch: 7290, G_Loss: 8.4165, D_Loss: 0.0003\n",
      "Epoch: 7300, G_Loss: 8.3253, D_Loss: 0.0003\n",
      "Epoch: 7310, G_Loss: 8.3471, D_Loss: 0.0003\n",
      "Epoch: 7320, G_Loss: 8.3980, D_Loss: 0.0003\n",
      "Epoch: 7330, G_Loss: 8.3288, D_Loss: 0.0004\n",
      "Epoch: 7340, G_Loss: 8.3519, D_Loss: 0.0003\n",
      "Epoch: 7350, G_Loss: 8.3424, D_Loss: 0.0003\n",
      "Epoch: 7360, G_Loss: 8.3943, D_Loss: 0.0003\n",
      "Epoch: 7370, G_Loss: 8.4486, D_Loss: 0.0002\n",
      "Epoch: 7380, G_Loss: 8.4957, D_Loss: 0.0003\n",
      "Epoch: 7390, G_Loss: 8.4882, D_Loss: 0.0002\n",
      "Epoch: 7400, G_Loss: 8.5337, D_Loss: 0.0002\n",
      "Epoch: 7410, G_Loss: 8.6215, D_Loss: 0.0002\n",
      "Epoch: 7420, G_Loss: 8.6120, D_Loss: 0.0002\n",
      "Epoch: 7430, G_Loss: 8.6424, D_Loss: 0.0002\n",
      "Epoch: 7440, G_Loss: 8.6290, D_Loss: 0.0002\n",
      "Epoch: 7450, G_Loss: 8.6582, D_Loss: 0.0002\n",
      "Epoch: 7460, G_Loss: 8.7247, D_Loss: 0.0002\n",
      "Epoch: 7470, G_Loss: 8.6741, D_Loss: 0.0002\n",
      "Epoch: 7480, G_Loss: 8.7472, D_Loss: 0.0002\n",
      "Epoch: 7490, G_Loss: 8.8214, D_Loss: 0.0002\n",
      "Epoch: 7500, G_Loss: 8.8074, D_Loss: 0.0002\n",
      "Epoch: 7510, G_Loss: 8.8063, D_Loss: 0.0002\n",
      "Epoch: 7520, G_Loss: 8.8003, D_Loss: 0.0002\n",
      "Epoch: 7530, G_Loss: 8.7504, D_Loss: 0.0002\n",
      "Epoch: 7540, G_Loss: 8.7719, D_Loss: 0.0002\n",
      "Epoch: 7550, G_Loss: 8.8582, D_Loss: 0.0002\n",
      "Epoch: 7560, G_Loss: 8.8203, D_Loss: 0.0002\n",
      "Epoch: 7570, G_Loss: 8.7642, D_Loss: 0.0002\n",
      "Epoch: 7580, G_Loss: 8.7877, D_Loss: 0.0002\n",
      "Epoch: 7590, G_Loss: 8.7940, D_Loss: 0.0002\n",
      "Epoch: 7600, G_Loss: 8.7570, D_Loss: 0.0002\n",
      "Epoch: 7610, G_Loss: 8.7077, D_Loss: 0.0002\n",
      "Epoch: 7620, G_Loss: 8.7414, D_Loss: 0.0002\n",
      "Epoch: 7630, G_Loss: 8.7712, D_Loss: 0.0002\n",
      "Epoch: 7640, G_Loss: 8.7484, D_Loss: 0.0002\n",
      "Epoch: 7650, G_Loss: 8.7726, D_Loss: 0.0002\n",
      "Epoch: 7660, G_Loss: 8.8292, D_Loss: 0.0002\n",
      "Epoch: 7670, G_Loss: 8.8462, D_Loss: 0.0002\n",
      "Epoch: 7680, G_Loss: 8.8287, D_Loss: 0.0002\n",
      "Epoch: 7690, G_Loss: 8.8159, D_Loss: 0.0002\n",
      "Epoch: 7700, G_Loss: 8.8220, D_Loss: 0.0002\n",
      "Epoch: 7710, G_Loss: 8.7233, D_Loss: 0.0002\n",
      "Epoch: 7720, G_Loss: 8.6545, D_Loss: 0.0002\n",
      "Epoch: 7730, G_Loss: 8.5824, D_Loss: 0.0002\n",
      "Epoch: 7740, G_Loss: 8.5579, D_Loss: 0.0002\n",
      "Epoch: 7750, G_Loss: 8.5205, D_Loss: 0.0002\n",
      "Epoch: 7760, G_Loss: 8.4731, D_Loss: 0.0002\n",
      "Epoch: 7770, G_Loss: 8.4064, D_Loss: 0.0002\n",
      "Epoch: 7780, G_Loss: 8.5601, D_Loss: 0.0002\n",
      "Epoch: 7790, G_Loss: 8.5147, D_Loss: 0.0002\n",
      "Epoch: 7800, G_Loss: 8.5355, D_Loss: 0.0002\n",
      "Epoch: 7810, G_Loss: 8.5823, D_Loss: 0.0002\n",
      "Epoch: 7820, G_Loss: 8.5472, D_Loss: 0.0002\n",
      "Epoch: 7830, G_Loss: 8.4810, D_Loss: 0.0002\n",
      "Epoch: 7840, G_Loss: 8.5860, D_Loss: 0.0002\n",
      "Epoch: 7850, G_Loss: 8.5959, D_Loss: 0.0002\n",
      "Epoch: 7860, G_Loss: 8.5562, D_Loss: 0.0002\n",
      "Epoch: 7870, G_Loss: 8.5884, D_Loss: 0.0002\n",
      "Epoch: 7880, G_Loss: 8.5950, D_Loss: 0.0002\n",
      "Epoch: 7890, G_Loss: 8.6624, D_Loss: 0.0002\n",
      "Epoch: 7900, G_Loss: 8.6764, D_Loss: 0.0002\n",
      "Epoch: 7910, G_Loss: 8.7169, D_Loss: 0.0002\n",
      "Epoch: 7920, G_Loss: 8.7381, D_Loss: 0.0002\n",
      "Epoch: 7930, G_Loss: 8.6663, D_Loss: 0.0002\n",
      "Epoch: 7940, G_Loss: 8.7172, D_Loss: 0.0002\n",
      "Epoch: 7950, G_Loss: 8.7280, D_Loss: 0.0002\n",
      "Epoch: 7960, G_Loss: 8.7462, D_Loss: 0.0002\n",
      "Epoch: 7970, G_Loss: 8.7091, D_Loss: 0.0002\n",
      "Epoch: 7980, G_Loss: 8.7877, D_Loss: 0.0002\n",
      "Epoch: 7990, G_Loss: 8.7477, D_Loss: 0.0002\n",
      "Epoch: 8000, G_Loss: 8.7469, D_Loss: 0.0002\n",
      "Epoch: 8010, G_Loss: 8.7446, D_Loss: 0.0002\n",
      "Epoch: 8020, G_Loss: 8.7765, D_Loss: 0.0002\n",
      "Epoch: 8030, G_Loss: 8.8237, D_Loss: 0.0002\n",
      "Epoch: 8040, G_Loss: 8.8404, D_Loss: 0.0002\n",
      "Epoch: 8050, G_Loss: 8.8100, D_Loss: 0.0002\n",
      "Epoch: 8060, G_Loss: 8.8928, D_Loss: 0.0002\n",
      "Epoch: 8070, G_Loss: 8.9326, D_Loss: 0.0001\n",
      "Epoch: 8080, G_Loss: 8.8746, D_Loss: 0.0001\n",
      "Epoch: 8090, G_Loss: 9.0082, D_Loss: 0.0001\n",
      "Epoch: 8100, G_Loss: 9.0034, D_Loss: 0.0001\n",
      "Epoch: 8110, G_Loss: 9.0538, D_Loss: 0.0001\n",
      "Epoch: 8120, G_Loss: 9.0469, D_Loss: 0.0001\n",
      "Epoch: 8130, G_Loss: 9.0744, D_Loss: 0.0001\n",
      "Epoch: 8140, G_Loss: 9.0405, D_Loss: 0.0001\n",
      "Epoch: 8150, G_Loss: 9.0770, D_Loss: 0.0001\n",
      "Epoch: 8160, G_Loss: 9.0545, D_Loss: 0.0001\n",
      "Epoch: 8170, G_Loss: 9.1744, D_Loss: 0.0001\n",
      "Epoch: 8180, G_Loss: 9.2130, D_Loss: 0.0001\n",
      "Epoch: 8190, G_Loss: 9.1181, D_Loss: 0.0001\n",
      "Epoch: 8200, G_Loss: 9.1998, D_Loss: 0.0001\n",
      "Epoch: 8210, G_Loss: 9.1723, D_Loss: 0.0001\n",
      "Epoch: 8220, G_Loss: 9.2115, D_Loss: 0.0001\n",
      "Epoch: 8230, G_Loss: 9.1841, D_Loss: 0.0001\n",
      "Epoch: 8240, G_Loss: 9.1934, D_Loss: 0.0001\n",
      "Epoch: 8250, G_Loss: 9.2869, D_Loss: 0.0001\n",
      "Epoch: 8260, G_Loss: 9.2589, D_Loss: 0.0001\n",
      "Epoch: 8270, G_Loss: 9.2607, D_Loss: 0.0001\n",
      "Epoch: 8280, G_Loss: 9.2350, D_Loss: 0.0001\n",
      "Epoch: 8290, G_Loss: 9.2634, D_Loss: 0.0001\n",
      "Epoch: 8300, G_Loss: 9.3423, D_Loss: 0.0001\n",
      "Epoch: 8310, G_Loss: 9.2295, D_Loss: 0.0001\n",
      "Epoch: 8320, G_Loss: 9.2324, D_Loss: 0.0001\n",
      "Epoch: 8330, G_Loss: 9.3706, D_Loss: 0.0001\n",
      "Epoch: 8340, G_Loss: 9.3118, D_Loss: 0.0001\n",
      "Epoch: 8350, G_Loss: 9.4611, D_Loss: 0.0001\n",
      "Epoch: 8360, G_Loss: 9.4497, D_Loss: 0.0001\n",
      "Epoch: 8370, G_Loss: 9.4749, D_Loss: 0.0001\n",
      "Epoch: 8380, G_Loss: 9.4718, D_Loss: 0.0001\n",
      "Epoch: 8390, G_Loss: 9.5111, D_Loss: 0.0001\n",
      "Epoch: 8400, G_Loss: 9.4810, D_Loss: 0.0001\n",
      "Epoch: 8410, G_Loss: 9.5428, D_Loss: 0.0001\n",
      "Epoch: 8420, G_Loss: 9.5083, D_Loss: 0.0001\n",
      "Epoch: 8430, G_Loss: 9.4994, D_Loss: 0.0001\n",
      "Epoch: 8440, G_Loss: 9.5324, D_Loss: 0.0001\n",
      "Epoch: 8450, G_Loss: 9.5953, D_Loss: 0.0001\n",
      "Epoch: 8460, G_Loss: 9.6238, D_Loss: 0.0001\n",
      "Epoch: 8470, G_Loss: 9.5580, D_Loss: 0.0001\n",
      "Epoch: 8480, G_Loss: 9.5274, D_Loss: 0.0001\n",
      "Epoch: 8490, G_Loss: 9.6195, D_Loss: 0.0001\n",
      "Epoch: 8500, G_Loss: 9.5942, D_Loss: 0.0001\n",
      "Epoch: 8510, G_Loss: 9.6063, D_Loss: 0.0001\n",
      "Epoch: 8520, G_Loss: 9.6595, D_Loss: 0.0001\n",
      "Epoch: 8530, G_Loss: 9.5991, D_Loss: 0.0001\n",
      "Epoch: 8540, G_Loss: 9.6652, D_Loss: 0.0001\n",
      "Epoch: 8550, G_Loss: 9.6323, D_Loss: 0.0001\n",
      "Epoch: 8560, G_Loss: 9.6453, D_Loss: 0.0001\n",
      "Epoch: 8570, G_Loss: 9.6635, D_Loss: 0.0001\n",
      "Epoch: 8580, G_Loss: 9.6554, D_Loss: 0.0001\n",
      "Epoch: 8590, G_Loss: 9.6962, D_Loss: 0.0001\n",
      "Epoch: 8600, G_Loss: 9.6877, D_Loss: 0.0001\n",
      "Epoch: 8610, G_Loss: 9.7128, D_Loss: 0.0001\n",
      "Epoch: 8620, G_Loss: 9.7275, D_Loss: 0.0001\n",
      "Epoch: 8630, G_Loss: 9.7163, D_Loss: 0.0001\n",
      "Epoch: 8640, G_Loss: 9.8008, D_Loss: 0.0001\n",
      "Epoch: 8650, G_Loss: 9.8301, D_Loss: 0.0001\n",
      "Epoch: 8660, G_Loss: 9.8264, D_Loss: 0.0001\n",
      "Epoch: 8670, G_Loss: 9.8353, D_Loss: 0.0001\n",
      "Epoch: 8680, G_Loss: 9.8504, D_Loss: 0.0001\n",
      "Epoch: 8690, G_Loss: 9.8082, D_Loss: 0.0001\n",
      "Epoch: 8700, G_Loss: 9.9036, D_Loss: 0.0001\n",
      "Epoch: 8710, G_Loss: 9.8941, D_Loss: 0.0001\n",
      "Epoch: 8720, G_Loss: 9.8954, D_Loss: 0.0001\n",
      "Epoch: 8730, G_Loss: 9.9858, D_Loss: 0.0001\n",
      "Epoch: 8740, G_Loss: 10.0430, D_Loss: 0.0001\n",
      "Epoch: 8750, G_Loss: 10.0801, D_Loss: 0.0001\n",
      "Epoch: 8760, G_Loss: 10.0995, D_Loss: 0.0001\n",
      "Epoch: 8770, G_Loss: 10.2189, D_Loss: 0.0000\n",
      "Epoch: 8780, G_Loss: 10.0819, D_Loss: 0.0001\n",
      "Epoch: 8790, G_Loss: 10.2512, D_Loss: 0.0001\n",
      "Epoch: 8800, G_Loss: 10.2200, D_Loss: 0.0001\n",
      "Epoch: 8810, G_Loss: 10.2459, D_Loss: 0.0001\n",
      "Epoch: 8820, G_Loss: 10.3937, D_Loss: 0.0000\n",
      "Epoch: 8830, G_Loss: 10.3483, D_Loss: 0.0000\n",
      "Epoch: 8840, G_Loss: 10.4070, D_Loss: 0.0001\n",
      "Epoch: 8850, G_Loss: 10.2994, D_Loss: 0.0001\n",
      "Epoch: 8860, G_Loss: 10.4473, D_Loss: 0.0001\n",
      "Epoch: 8870, G_Loss: 10.4174, D_Loss: 0.0001\n",
      "Epoch: 8880, G_Loss: 10.4191, D_Loss: 0.0000\n",
      "Epoch: 8890, G_Loss: 10.4188, D_Loss: 0.0000\n",
      "Epoch: 8900, G_Loss: 10.5232, D_Loss: 0.0000\n",
      "Epoch: 8910, G_Loss: 10.7147, D_Loss: 0.0001\n",
      "Epoch: 8920, G_Loss: 10.7948, D_Loss: 0.0000\n",
      "Epoch: 8930, G_Loss: 10.7603, D_Loss: 0.0000\n",
      "Epoch: 8940, G_Loss: 10.6272, D_Loss: 0.0000\n",
      "Epoch: 8950, G_Loss: 10.7103, D_Loss: 0.0000\n",
      "Epoch: 8960, G_Loss: 10.8158, D_Loss: 0.0000\n",
      "Epoch: 8970, G_Loss: 10.7284, D_Loss: 0.0000\n",
      "Epoch: 8980, G_Loss: 10.7366, D_Loss: 0.0000\n",
      "Epoch: 8990, G_Loss: 10.7773, D_Loss: 0.0000\n",
      "Epoch: 9000, G_Loss: 10.6601, D_Loss: 0.0000\n",
      "Epoch: 9010, G_Loss: 10.4998, D_Loss: 0.0001\n",
      "Epoch: 9020, G_Loss: 10.4686, D_Loss: 0.0001\n",
      "Epoch: 9030, G_Loss: 10.4940, D_Loss: 0.0000\n",
      "Epoch: 9040, G_Loss: 10.6244, D_Loss: 0.0000\n",
      "Epoch: 9050, G_Loss: 10.6449, D_Loss: 0.0000\n",
      "Epoch: 9060, G_Loss: 10.6416, D_Loss: 0.0001\n",
      "Epoch: 9070, G_Loss: 10.6262, D_Loss: 0.0001\n",
      "Epoch: 9080, G_Loss: 10.6719, D_Loss: 0.0000\n",
      "Epoch: 9090, G_Loss: 10.6172, D_Loss: 0.0000\n",
      "Epoch: 9100, G_Loss: 10.6431, D_Loss: 0.0000\n",
      "Epoch: 9110, G_Loss: 10.5402, D_Loss: 0.0001\n",
      "Epoch: 9120, G_Loss: 10.7469, D_Loss: 0.0000\n",
      "Epoch: 9130, G_Loss: 10.8790, D_Loss: 0.0000\n",
      "Epoch: 9140, G_Loss: 10.7996, D_Loss: 0.0000\n",
      "Epoch: 9150, G_Loss: 10.7415, D_Loss: 0.0000\n",
      "Epoch: 9160, G_Loss: 10.8140, D_Loss: 0.0000\n",
      "Epoch: 9170, G_Loss: 10.7897, D_Loss: 0.0000\n",
      "Epoch: 9180, G_Loss: 10.7265, D_Loss: 0.0000\n",
      "Epoch: 9190, G_Loss: 10.6517, D_Loss: 0.0000\n",
      "Epoch: 9200, G_Loss: 10.6063, D_Loss: 0.0000\n",
      "Epoch: 9210, G_Loss: 10.6457, D_Loss: 0.0000\n",
      "Epoch: 9220, G_Loss: 10.6940, D_Loss: 0.0000\n",
      "Epoch: 9230, G_Loss: 10.6912, D_Loss: 0.0001\n",
      "Epoch: 9240, G_Loss: 10.5298, D_Loss: 0.0000\n",
      "Epoch: 9250, G_Loss: 10.4656, D_Loss: 0.0001\n",
      "Epoch: 9260, G_Loss: 10.5120, D_Loss: 0.0000\n",
      "Epoch: 9270, G_Loss: 10.5221, D_Loss: 0.0001\n",
      "Epoch: 9280, G_Loss: 10.5343, D_Loss: 0.0001\n",
      "Epoch: 9290, G_Loss: 10.5130, D_Loss: 0.0000\n",
      "Epoch: 9300, G_Loss: 10.5879, D_Loss: 0.0000\n",
      "Epoch: 9310, G_Loss: 10.5172, D_Loss: 0.0000\n",
      "Epoch: 9320, G_Loss: 10.4649, D_Loss: 0.0001\n",
      "Epoch: 9330, G_Loss: 10.5224, D_Loss: 0.0001\n",
      "Epoch: 9340, G_Loss: 10.6184, D_Loss: 0.0000\n",
      "Epoch: 9350, G_Loss: 10.5911, D_Loss: 0.0000\n",
      "Epoch: 9360, G_Loss: 10.5763, D_Loss: 0.0000\n",
      "Epoch: 9370, G_Loss: 10.5697, D_Loss: 0.0000\n",
      "Epoch: 9380, G_Loss: 10.4878, D_Loss: 0.0000\n",
      "Epoch: 9390, G_Loss: 10.5366, D_Loss: 0.0001\n",
      "Epoch: 9400, G_Loss: 10.3539, D_Loss: 0.0001\n",
      "Epoch: 9410, G_Loss: 10.4606, D_Loss: 0.0001\n",
      "Epoch: 9420, G_Loss: 10.4384, D_Loss: 0.0001\n",
      "Epoch: 9430, G_Loss: 10.4064, D_Loss: 0.0001\n",
      "Epoch: 9440, G_Loss: 10.4562, D_Loss: 0.0001\n",
      "Epoch: 9450, G_Loss: 10.4193, D_Loss: 0.0001\n",
      "Epoch: 9460, G_Loss: 10.4289, D_Loss: 0.0001\n",
      "Epoch: 9470, G_Loss: 10.3541, D_Loss: 0.0001\n",
      "Epoch: 9480, G_Loss: 10.3586, D_Loss: 0.0001\n",
      "Epoch: 9490, G_Loss: 10.3493, D_Loss: 0.0001\n",
      "Epoch: 9500, G_Loss: 10.2683, D_Loss: 0.0001\n",
      "Epoch: 9510, G_Loss: 10.1004, D_Loss: 0.0001\n",
      "Epoch: 9520, G_Loss: 10.3128, D_Loss: 0.0001\n",
      "Epoch: 9530, G_Loss: 10.3205, D_Loss: 0.0001\n",
      "Epoch: 9540, G_Loss: 10.3841, D_Loss: 0.0001\n",
      "Epoch: 9550, G_Loss: 10.3071, D_Loss: 0.0001\n",
      "Epoch: 9560, G_Loss: 10.4704, D_Loss: 0.0001\n",
      "Epoch: 9570, G_Loss: 10.5804, D_Loss: 0.0001\n",
      "Epoch: 9580, G_Loss: 10.6309, D_Loss: 0.0001\n",
      "Epoch: 9590, G_Loss: 10.6314, D_Loss: 0.0001\n",
      "Epoch: 9600, G_Loss: 10.4956, D_Loss: 0.0001\n",
      "Epoch: 9610, G_Loss: 10.3862, D_Loss: 0.0001\n",
      "Epoch: 9620, G_Loss: 10.4264, D_Loss: 0.0001\n",
      "Epoch: 9630, G_Loss: 10.4511, D_Loss: 0.0001\n",
      "Epoch: 9640, G_Loss: 10.3978, D_Loss: 0.0001\n",
      "Epoch: 9650, G_Loss: 10.4710, D_Loss: 0.0001\n",
      "Epoch: 9660, G_Loss: 10.4139, D_Loss: 0.0001\n",
      "Epoch: 9670, G_Loss: 10.4744, D_Loss: 0.0001\n",
      "Epoch: 9680, G_Loss: 10.4268, D_Loss: 0.0001\n",
      "Epoch: 9690, G_Loss: 10.3233, D_Loss: 0.0001\n",
      "Epoch: 9700, G_Loss: 10.3435, D_Loss: 0.0001\n",
      "Epoch: 9710, G_Loss: 10.3905, D_Loss: 0.0001\n",
      "Epoch: 9720, G_Loss: 10.4671, D_Loss: 0.0001\n",
      "Epoch: 9730, G_Loss: 10.4579, D_Loss: 0.0001\n",
      "Epoch: 9740, G_Loss: 10.4436, D_Loss: 0.0001\n",
      "Epoch: 9750, G_Loss: 10.4696, D_Loss: 0.0001\n",
      "Epoch: 9760, G_Loss: 10.4692, D_Loss: 0.0001\n",
      "Epoch: 9770, G_Loss: 10.5502, D_Loss: 0.0001\n",
      "Epoch: 9780, G_Loss: 10.5044, D_Loss: 0.0001\n",
      "Epoch: 9790, G_Loss: 10.5965, D_Loss: 0.0001\n",
      "Epoch: 9800, G_Loss: 10.7187, D_Loss: 0.0000\n",
      "Epoch: 9810, G_Loss: 10.6984, D_Loss: 0.0001\n",
      "Epoch: 9820, G_Loss: 10.7382, D_Loss: 0.0001\n",
      "Epoch: 9830, G_Loss: 10.6129, D_Loss: 0.0001\n",
      "Epoch: 9840, G_Loss: 10.4508, D_Loss: 0.0001\n",
      "Epoch: 9850, G_Loss: 10.4702, D_Loss: 0.0001\n",
      "Epoch: 9860, G_Loss: 10.5891, D_Loss: 0.0001\n",
      "Epoch: 9870, G_Loss: 10.3303, D_Loss: 0.0001\n",
      "Epoch: 9880, G_Loss: 10.3886, D_Loss: 0.0001\n",
      "Epoch: 9890, G_Loss: 10.4562, D_Loss: 0.0001\n",
      "Epoch: 9900, G_Loss: 10.4534, D_Loss: 0.0001\n",
      "Epoch: 9910, G_Loss: 10.5655, D_Loss: 0.0001\n",
      "Epoch: 9920, G_Loss: 10.6145, D_Loss: 0.0001\n",
      "Epoch: 9930, G_Loss: 10.6674, D_Loss: 0.0001\n",
      "Epoch: 9940, G_Loss: 10.6437, D_Loss: 0.0000\n",
      "Epoch: 9950, G_Loss: 10.6799, D_Loss: 0.0000\n",
      "Epoch: 9960, G_Loss: 10.7196, D_Loss: 0.0000\n",
      "Epoch: 9970, G_Loss: 10.7370, D_Loss: 0.0000\n",
      "Epoch: 9980, G_Loss: 10.7522, D_Loss: 0.0000\n",
      "Epoch: 9990, G_Loss: 10.8166, D_Loss: 0.0000\n"
     ]
    }
   ],
   "source": [
    "discriminator.train()\n",
    "generator.train()\n",
    "min_loss = 1000\n",
    "for epoch in range(10000):\n",
    "    for data in gan_dataloader:\n",
    "        # print(\"********\")\n",
    "        real_data, _ = data\n",
    "        real_data = real_data.to(device)\n",
    "        # 训练判别器\n",
    "        d_optimizer.zero_grad()\n",
    "        real_label = torch.ones(real_data.shape[0], 1).to(device)\n",
    "\n",
    "        fake_data = generator(torch.randn(real_data.shape[0], input_dim).to(device)).detach()\n",
    "        fake_label = torch.zeros(real_data.shape[0], 1).to(device)\n",
    "        real_out = discriminator(real_data)\n",
    "        fake_out = discriminator(fake_data)\n",
    "        real_loss = loss_func(real_out, real_label)\n",
    "        fake_loss = loss_func(fake_out, fake_label)\n",
    "        d_loss = real_loss + fake_loss\n",
    "        d_loss.backward(retain_graph=True)\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # 训练生成器\n",
    "        g_optimizer.zero_grad()\n",
    "        gen_input = torch.randn(real_data.shape[0], input_dim).to(device)\n",
    "        gen_output = generator(gen_input)\n",
    "        dis_output = discriminator(gen_output)\n",
    "\n",
    "        g_loss = loss_func(dis_output, real_label)\n",
    "\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "        if g_loss.item() < min_loss:\n",
    "            torch.save(generator.state_dict(), \"generator_0.pth\")\n",
    "    if epoch % 10 == 0:\n",
    "        print(\"Epoch: {}, G_Loss: {:.4f}, D_Loss: {:.4f}\".format(epoch, g_loss.item(), d_loss.item()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f87afe8f-37c3-4a61-bdce-d2d9a6bf1951",
   "metadata": {},
   "source": [
    "# 生成样本数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77fa1dfc-6c0e-48c2-9501-28c85f0c89b1",
   "metadata": {},
   "source": [
    "## 查看需要生成多少样本数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c6f32a7b-cbd4-4170-bb7a-7a4a8a489f73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DDR1</th>\n",
       "      <th>RFC2</th>\n",
       "      <th>HSPA6</th>\n",
       "      <th>PAX8</th>\n",
       "      <th>GUCA1A</th>\n",
       "      <th>MIR5193</th>\n",
       "      <th>THRA</th>\n",
       "      <th>PTPN21</th>\n",
       "      <th>CCL5</th>\n",
       "      <th>CYP2E1</th>\n",
       "      <th>...</th>\n",
       "      <th>MINOS1-NBL1</th>\n",
       "      <th>NUS1P3</th>\n",
       "      <th>MROH7-TTC4</th>\n",
       "      <th>RBM12B</th>\n",
       "      <th>LOC102725263</th>\n",
       "      <th>FAM86B1</th>\n",
       "      <th>SNHG17</th>\n",
       "      <th>LOC100505915</th>\n",
       "      <th>NPEPL1</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.828700</td>\n",
       "      <td>9.523877</td>\n",
       "      <td>7.907263</td>\n",
       "      <td>10.750475</td>\n",
       "      <td>6.478643</td>\n",
       "      <td>8.698516</td>\n",
       "      <td>7.526617</td>\n",
       "      <td>7.276040</td>\n",
       "      <td>10.399704</td>\n",
       "      <td>6.642930</td>\n",
       "      <td>...</td>\n",
       "      <td>11.356547</td>\n",
       "      <td>6.894852</td>\n",
       "      <td>9.188312</td>\n",
       "      <td>8.628715</td>\n",
       "      <td>9.029101</td>\n",
       "      <td>8.669479</td>\n",
       "      <td>8.573230</td>\n",
       "      <td>5.535360</td>\n",
       "      <td>7.364660</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.241262</td>\n",
       "      <td>8.543289</td>\n",
       "      <td>8.435700</td>\n",
       "      <td>11.931321</td>\n",
       "      <td>7.151195</td>\n",
       "      <td>10.130569</td>\n",
       "      <td>9.457966</td>\n",
       "      <td>7.663284</td>\n",
       "      <td>6.242512</td>\n",
       "      <td>6.902703</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920599</td>\n",
       "      <td>1.356235</td>\n",
       "      <td>8.111887</td>\n",
       "      <td>8.679902</td>\n",
       "      <td>9.154389</td>\n",
       "      <td>8.743268</td>\n",
       "      <td>9.011958</td>\n",
       "      <td>4.717732</td>\n",
       "      <td>9.323271</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.993611</td>\n",
       "      <td>8.004977</td>\n",
       "      <td>8.741661</td>\n",
       "      <td>11.579397</td>\n",
       "      <td>7.433843</td>\n",
       "      <td>9.621802</td>\n",
       "      <td>8.319854</td>\n",
       "      <td>7.718079</td>\n",
       "      <td>5.457564</td>\n",
       "      <td>6.741639</td>\n",
       "      <td>...</td>\n",
       "      <td>10.215395</td>\n",
       "      <td>5.618485</td>\n",
       "      <td>9.432083</td>\n",
       "      <td>7.999077</td>\n",
       "      <td>8.554622</td>\n",
       "      <td>9.404803</td>\n",
       "      <td>7.887245</td>\n",
       "      <td>7.180957</td>\n",
       "      <td>8.758899</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.028597</td>\n",
       "      <td>8.889190</td>\n",
       "      <td>7.975727</td>\n",
       "      <td>10.501003</td>\n",
       "      <td>7.320102</td>\n",
       "      <td>9.473202</td>\n",
       "      <td>8.133202</td>\n",
       "      <td>5.137284</td>\n",
       "      <td>8.062176</td>\n",
       "      <td>6.796330</td>\n",
       "      <td>...</td>\n",
       "      <td>10.992724</td>\n",
       "      <td>6.310822</td>\n",
       "      <td>9.204276</td>\n",
       "      <td>9.277398</td>\n",
       "      <td>10.488880</td>\n",
       "      <td>8.439994</td>\n",
       "      <td>9.210394</td>\n",
       "      <td>5.208338</td>\n",
       "      <td>7.218112</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.275150</td>\n",
       "      <td>7.982162</td>\n",
       "      <td>7.971315</td>\n",
       "      <td>11.017791</td>\n",
       "      <td>6.932531</td>\n",
       "      <td>9.590634</td>\n",
       "      <td>8.446712</td>\n",
       "      <td>5.407080</td>\n",
       "      <td>10.304378</td>\n",
       "      <td>7.145151</td>\n",
       "      <td>...</td>\n",
       "      <td>10.871971</td>\n",
       "      <td>5.364275</td>\n",
       "      <td>9.003275</td>\n",
       "      <td>8.045155</td>\n",
       "      <td>9.305313</td>\n",
       "      <td>8.852204</td>\n",
       "      <td>9.044612</td>\n",
       "      <td>6.692939</td>\n",
       "      <td>7.591082</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>483</th>\n",
       "      <td>11.936811</td>\n",
       "      <td>8.196444</td>\n",
       "      <td>8.358999</td>\n",
       "      <td>10.438085</td>\n",
       "      <td>6.986401</td>\n",
       "      <td>8.947285</td>\n",
       "      <td>7.340735</td>\n",
       "      <td>7.729171</td>\n",
       "      <td>6.857256</td>\n",
       "      <td>6.910038</td>\n",
       "      <td>...</td>\n",
       "      <td>11.448681</td>\n",
       "      <td>5.899329</td>\n",
       "      <td>8.921185</td>\n",
       "      <td>7.470472</td>\n",
       "      <td>9.912844</td>\n",
       "      <td>8.272279</td>\n",
       "      <td>8.474885</td>\n",
       "      <td>7.160992</td>\n",
       "      <td>7.249352</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>484</th>\n",
       "      <td>11.743730</td>\n",
       "      <td>9.206188</td>\n",
       "      <td>8.704715</td>\n",
       "      <td>10.817337</td>\n",
       "      <td>7.556436</td>\n",
       "      <td>9.359459</td>\n",
       "      <td>7.818077</td>\n",
       "      <td>7.218218</td>\n",
       "      <td>8.007647</td>\n",
       "      <td>6.902983</td>\n",
       "      <td>...</td>\n",
       "      <td>9.540868</td>\n",
       "      <td>5.655864</td>\n",
       "      <td>8.995773</td>\n",
       "      <td>8.154563</td>\n",
       "      <td>8.870963</td>\n",
       "      <td>8.862683</td>\n",
       "      <td>8.833386</td>\n",
       "      <td>6.583812</td>\n",
       "      <td>8.607642</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>485</th>\n",
       "      <td>11.763716</td>\n",
       "      <td>8.675637</td>\n",
       "      <td>8.216023</td>\n",
       "      <td>10.596750</td>\n",
       "      <td>6.840312</td>\n",
       "      <td>9.642787</td>\n",
       "      <td>7.412919</td>\n",
       "      <td>7.579046</td>\n",
       "      <td>7.749030</td>\n",
       "      <td>6.708932</td>\n",
       "      <td>...</td>\n",
       "      <td>10.964748</td>\n",
       "      <td>7.051643</td>\n",
       "      <td>8.625504</td>\n",
       "      <td>8.041096</td>\n",
       "      <td>9.241797</td>\n",
       "      <td>8.762348</td>\n",
       "      <td>8.793579</td>\n",
       "      <td>5.173663</td>\n",
       "      <td>7.033699</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>486</th>\n",
       "      <td>10.838269</td>\n",
       "      <td>7.985254</td>\n",
       "      <td>8.092749</td>\n",
       "      <td>11.253446</td>\n",
       "      <td>7.598196</td>\n",
       "      <td>9.082484</td>\n",
       "      <td>8.112567</td>\n",
       "      <td>7.139708</td>\n",
       "      <td>9.324205</td>\n",
       "      <td>7.000705</td>\n",
       "      <td>...</td>\n",
       "      <td>12.008883</td>\n",
       "      <td>5.553036</td>\n",
       "      <td>9.147902</td>\n",
       "      <td>6.768057</td>\n",
       "      <td>8.748552</td>\n",
       "      <td>8.072324</td>\n",
       "      <td>8.026667</td>\n",
       "      <td>6.329411</td>\n",
       "      <td>7.726061</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>12.527029</td>\n",
       "      <td>7.558050</td>\n",
       "      <td>8.047053</td>\n",
       "      <td>11.510339</td>\n",
       "      <td>6.828349</td>\n",
       "      <td>9.235923</td>\n",
       "      <td>7.944802</td>\n",
       "      <td>4.982382</td>\n",
       "      <td>6.013900</td>\n",
       "      <td>6.393754</td>\n",
       "      <td>...</td>\n",
       "      <td>11.939225</td>\n",
       "      <td>6.171831</td>\n",
       "      <td>9.076117</td>\n",
       "      <td>7.187029</td>\n",
       "      <td>9.539517</td>\n",
       "      <td>8.363781</td>\n",
       "      <td>9.452459</td>\n",
       "      <td>5.546775</td>\n",
       "      <td>7.888972</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>488 rows × 13237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DDR1      RFC2     HSPA6       PAX8    GUCA1A    MIR5193      THRA   \n",
       "0    11.828700  9.523877  7.907263  10.750475  6.478643   8.698516  7.526617  \\\n",
       "1    13.241262  8.543289  8.435700  11.931321  7.151195  10.130569  9.457966   \n",
       "2    12.993611  8.004977  8.741661  11.579397  7.433843   9.621802  8.319854   \n",
       "3    12.028597  8.889190  7.975727  10.501003  7.320102   9.473202  8.133202   \n",
       "4    12.275150  7.982162  7.971315  11.017791  6.932531   9.590634  8.446712   \n",
       "..         ...       ...       ...        ...       ...        ...       ...   \n",
       "483  11.936811  8.196444  8.358999  10.438085  6.986401   8.947285  7.340735   \n",
       "484  11.743730  9.206188  8.704715  10.817337  7.556436   9.359459  7.818077   \n",
       "485  11.763716  8.675637  8.216023  10.596750  6.840312   9.642787  7.412919   \n",
       "486  10.838269  7.985254  8.092749  11.253446  7.598196   9.082484  8.112567   \n",
       "487  12.527029  7.558050  8.047053  11.510339  6.828349   9.235923  7.944802   \n",
       "\n",
       "       PTPN21       CCL5    CYP2E1  ...  MINOS1-NBL1    NUS1P3  MROH7-TTC4   \n",
       "0    7.276040  10.399704  6.642930  ...    11.356547  6.894852    9.188312  \\\n",
       "1    7.663284   6.242512  6.902703  ...    11.920599  1.356235    8.111887   \n",
       "2    7.718079   5.457564  6.741639  ...    10.215395  5.618485    9.432083   \n",
       "3    5.137284   8.062176  6.796330  ...    10.992724  6.310822    9.204276   \n",
       "4    5.407080  10.304378  7.145151  ...    10.871971  5.364275    9.003275   \n",
       "..        ...        ...       ...  ...          ...       ...         ...   \n",
       "483  7.729171   6.857256  6.910038  ...    11.448681  5.899329    8.921185   \n",
       "484  7.218218   8.007647  6.902983  ...     9.540868  5.655864    8.995773   \n",
       "485  7.579046   7.749030  6.708932  ...    10.964748  7.051643    8.625504   \n",
       "486  7.139708   9.324205  7.000705  ...    12.008883  5.553036    9.147902   \n",
       "487  4.982382   6.013900  6.393754  ...    11.939225  6.171831    9.076117   \n",
       "\n",
       "       RBM12B  LOC102725263   FAM86B1    SNHG17  LOC100505915    NPEPL1  group  \n",
       "0    8.628715      9.029101  8.669479  8.573230      5.535360  7.364660      0  \n",
       "1    8.679902      9.154389  8.743268  9.011958      4.717732  9.323271      0  \n",
       "2    7.999077      8.554622  9.404803  7.887245      7.180957  8.758899      0  \n",
       "3    9.277398     10.488880  8.439994  9.210394      5.208338  7.218112      0  \n",
       "4    8.045155      9.305313  8.852204  9.044612      6.692939  7.591082      0  \n",
       "..        ...           ...       ...       ...           ...       ...    ...  \n",
       "483  7.470472      9.912844  8.272279  8.474885      7.160992  7.249352      1  \n",
       "484  8.154563      8.870963  8.862683  8.833386      6.583812  8.607642      1  \n",
       "485  8.041096      9.241797  8.762348  8.793579      5.173663  7.033699      1  \n",
       "486  6.768057      8.748552  8.072324  8.026667      6.329411  7.726061      1  \n",
       "487  7.187029      9.539517  8.363781  9.452459      5.546775  7.888972      1  \n",
       "\n",
       "[488 rows x 13237 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSE25066_data = pd.read_csv('GSE25066_merge.csv')\n",
    "GSE25066_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "67ccc8c8-cc9c-4a37-8902-2bd946aaa765",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "290"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_num = len(GSE25066_data[GSE25066_data['group'] == 1]) - len(GSE25066_data[GSE25066_data['group'] == 0])\n",
    "gen_num"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "a169c016-d194-46b1-9b59-1ceb92fd6dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([290, 64])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_z = torch.randn(gen_num, input_dim).to(device)\n",
    "gen_z.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6b6843d8-de03-4de3-b3e9-0ccdc400b06f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0712, -0.4303, -0.4074,  ..., -0.4893,  0.9662, -0.1570],\n",
       "        [ 0.0916, -0.4446, -0.2520,  ..., -0.4837,  0.9743,  0.0898],\n",
       "        [ 0.0192, -0.5061, -0.3333,  ..., -0.4325,  0.9630, -0.0148],\n",
       "        ...,\n",
       "        [ 0.0377, -0.5254, -0.2750,  ..., -0.4627,  0.9627, -0.1331],\n",
       "        [-0.0554, -0.5146, -0.2699,  ..., -0.3765,  0.9422,  0.0330],\n",
       "        [-0.1753, -0.3198, -0.3283,  ..., -0.4824,  0.9643, -0.1702]],\n",
       "       device='cuda:0', grad_fn=<TanhBackward0>)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_data = generator(gen_z)\n",
    "gen_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae24d81e-d812-46f0-86db-b4dca6c74982",
   "metadata": {},
   "source": [
    "## 解码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0d87c380-7c1c-4c63-8250-09ec4538d44b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5488, 0.4889, 0.4783,  ..., 0.4885, 0.4670, 0.4912],\n",
       "        [0.5461, 0.4887, 0.4768,  ..., 0.4886, 0.4673, 0.4842],\n",
       "        [0.5514, 0.4869, 0.4808,  ..., 0.4955, 0.4703, 0.4812],\n",
       "        ...,\n",
       "        [0.5543, 0.4882, 0.4829,  ..., 0.4995, 0.4617, 0.4919],\n",
       "        [0.5494, 0.4882, 0.4816,  ..., 0.4910, 0.4695, 0.4931],\n",
       "        [0.5478, 0.4854, 0.4799,  ..., 0.4932, 0.4606, 0.4827]],\n",
       "       device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_normal_data = model.decoder(gen_data)\n",
    "gen_normal_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd498bf4-a905-4ff0-aab0-05f2a7d92d1c",
   "metadata": {},
   "source": [
    "## minmax反归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f5c5b805-2dc4-4788-b167-7dabe79228b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[10.3133,  8.9024,  8.6536,  ...,  8.8925,  8.3868,  8.9561],\n",
       "        [10.2505,  8.8966,  8.6180,  ...,  8.8948,  8.3936,  8.7923],\n",
       "        [10.3737,  8.8551,  8.7103,  ...,  9.0584,  8.4636,  8.7215],\n",
       "        ...,\n",
       "        [10.4431,  8.8861,  8.7620,  ...,  9.1514,  8.2624,  8.9730],\n",
       "        [10.3276,  8.8863,  8.7310,  ...,  8.9520,  8.4444,  9.0019],\n",
       "        [10.2894,  8.8209,  8.6894,  ...,  9.0030,  8.2355,  8.7554]],\n",
       "       device='cuda:0', grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_denormal_data = min_max_denormalize(gen_normal_data, ae_min, ae_max)\n",
    "gen_denormal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9c29f06b-98c5-45d6-accf-e6a2d7a1d2ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[10.31335  ,  8.902443 ,  8.653562 , ...,  8.892497 ,  8.386763 ,\n",
       "         8.956055 ],\n",
       "       [10.250475 ,  8.896602 ,  8.618033 , ...,  8.894805 ,  8.393645 ,\n",
       "         8.79229  ],\n",
       "       [10.3737335,  8.855096 ,  8.710294 , ...,  9.058387 ,  8.463587 ,\n",
       "         8.721526 ],\n",
       "       ...,\n",
       "       [10.443093 ,  8.886114 ,  8.761951 , ...,  9.151364 ,  8.26241  ,\n",
       "         8.972986 ],\n",
       "       [10.327574 ,  8.88632  ,  8.731006 , ...,  8.952026 ,  8.444355 ,\n",
       "         9.001902 ],\n",
       "       [10.289376 ,  8.820926 ,  8.689403 , ...,  9.003032 ,  8.235528 ,\n",
       "         8.755377 ]], dtype=float32)"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gen_data_np = gen_denormal_data.cpu().detach().numpy()\n",
    "gen_data_np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bf7bf0ba-18b1-426e-82da-df17d473e7a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[10.31334972,  8.90244293,  8.65356159, ...,  8.38676262,\n",
       "          8.95605469,  0.        ],\n",
       "        [10.25047493,  8.89660168,  8.61803341, ...,  8.39364529,\n",
       "          8.79228973,  0.        ],\n",
       "        [10.37373352,  8.85509586,  8.71029377, ...,  8.46358681,\n",
       "          8.72152615,  0.        ],\n",
       "        ...,\n",
       "        [10.4430933 ,  8.88611412,  8.76195145, ...,  8.26241016,\n",
       "          8.97298622,  0.        ],\n",
       "        [10.32757378,  8.88632011,  8.73100567, ...,  8.44435501,\n",
       "          9.00190163,  0.        ],\n",
       "        [10.28937626,  8.82092571,  8.68940258, ...,  8.23552799,\n",
       "          8.75537682,  0.        ]]),\n",
       " (290, 13237))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros_column = np.zeros((gen_data_np.shape[0], 1))\n",
    "\n",
    "# 将列向量与原始数组水平堆叠\n",
    "gen_data_np = np.hstack((gen_data_np, zeros_column))\n",
    "gen_data_np, gen_data_np.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "c1caae53-6f1a-4875-b2fb-f98e7c7bf86f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将NumPy数组转换为DataFrame\n",
    "arr_df = pd.DataFrame(gen_data_np, columns=GSE25066_data.columns.values)\n",
    "\n",
    "# 将两个DataFrame对象沿着行的方向连接\n",
    "new_GSE25066_df = pd.concat([GSE25066_data, arr_df], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e7381ff4-75f2-4681-b3aa-79ec98709937",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DDR1</th>\n",
       "      <th>RFC2</th>\n",
       "      <th>HSPA6</th>\n",
       "      <th>PAX8</th>\n",
       "      <th>GUCA1A</th>\n",
       "      <th>MIR5193</th>\n",
       "      <th>THRA</th>\n",
       "      <th>PTPN21</th>\n",
       "      <th>CCL5</th>\n",
       "      <th>CYP2E1</th>\n",
       "      <th>...</th>\n",
       "      <th>MINOS1-NBL1</th>\n",
       "      <th>NUS1P3</th>\n",
       "      <th>MROH7-TTC4</th>\n",
       "      <th>RBM12B</th>\n",
       "      <th>LOC102725263</th>\n",
       "      <th>FAM86B1</th>\n",
       "      <th>SNHG17</th>\n",
       "      <th>LOC100505915</th>\n",
       "      <th>NPEPL1</th>\n",
       "      <th>group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>11.828700</td>\n",
       "      <td>9.523877</td>\n",
       "      <td>7.907263</td>\n",
       "      <td>10.750475</td>\n",
       "      <td>6.478643</td>\n",
       "      <td>8.698516</td>\n",
       "      <td>7.526617</td>\n",
       "      <td>7.276040</td>\n",
       "      <td>10.399704</td>\n",
       "      <td>6.642930</td>\n",
       "      <td>...</td>\n",
       "      <td>11.356547</td>\n",
       "      <td>6.894852</td>\n",
       "      <td>9.188312</td>\n",
       "      <td>8.628715</td>\n",
       "      <td>9.029101</td>\n",
       "      <td>8.669479</td>\n",
       "      <td>8.573230</td>\n",
       "      <td>5.535360</td>\n",
       "      <td>7.364660</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.241262</td>\n",
       "      <td>8.543289</td>\n",
       "      <td>8.435700</td>\n",
       "      <td>11.931321</td>\n",
       "      <td>7.151195</td>\n",
       "      <td>10.130569</td>\n",
       "      <td>9.457966</td>\n",
       "      <td>7.663284</td>\n",
       "      <td>6.242512</td>\n",
       "      <td>6.902703</td>\n",
       "      <td>...</td>\n",
       "      <td>11.920599</td>\n",
       "      <td>1.356235</td>\n",
       "      <td>8.111887</td>\n",
       "      <td>8.679902</td>\n",
       "      <td>9.154389</td>\n",
       "      <td>8.743268</td>\n",
       "      <td>9.011958</td>\n",
       "      <td>4.717732</td>\n",
       "      <td>9.323271</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12.993611</td>\n",
       "      <td>8.004977</td>\n",
       "      <td>8.741661</td>\n",
       "      <td>11.579397</td>\n",
       "      <td>7.433843</td>\n",
       "      <td>9.621802</td>\n",
       "      <td>8.319854</td>\n",
       "      <td>7.718079</td>\n",
       "      <td>5.457564</td>\n",
       "      <td>6.741639</td>\n",
       "      <td>...</td>\n",
       "      <td>10.215395</td>\n",
       "      <td>5.618485</td>\n",
       "      <td>9.432083</td>\n",
       "      <td>7.999077</td>\n",
       "      <td>8.554622</td>\n",
       "      <td>9.404803</td>\n",
       "      <td>7.887245</td>\n",
       "      <td>7.180957</td>\n",
       "      <td>8.758899</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>12.028597</td>\n",
       "      <td>8.889190</td>\n",
       "      <td>7.975727</td>\n",
       "      <td>10.501003</td>\n",
       "      <td>7.320102</td>\n",
       "      <td>9.473202</td>\n",
       "      <td>8.133202</td>\n",
       "      <td>5.137284</td>\n",
       "      <td>8.062176</td>\n",
       "      <td>6.796330</td>\n",
       "      <td>...</td>\n",
       "      <td>10.992724</td>\n",
       "      <td>6.310822</td>\n",
       "      <td>9.204276</td>\n",
       "      <td>9.277398</td>\n",
       "      <td>10.488880</td>\n",
       "      <td>8.439994</td>\n",
       "      <td>9.210394</td>\n",
       "      <td>5.208338</td>\n",
       "      <td>7.218112</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12.275150</td>\n",
       "      <td>7.982162</td>\n",
       "      <td>7.971315</td>\n",
       "      <td>11.017791</td>\n",
       "      <td>6.932531</td>\n",
       "      <td>9.590634</td>\n",
       "      <td>8.446712</td>\n",
       "      <td>5.407080</td>\n",
       "      <td>10.304378</td>\n",
       "      <td>7.145151</td>\n",
       "      <td>...</td>\n",
       "      <td>10.871971</td>\n",
       "      <td>5.364275</td>\n",
       "      <td>9.003275</td>\n",
       "      <td>8.045155</td>\n",
       "      <td>9.305313</td>\n",
       "      <td>8.852204</td>\n",
       "      <td>9.044612</td>\n",
       "      <td>6.692939</td>\n",
       "      <td>7.591082</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>10.501408</td>\n",
       "      <td>9.016302</td>\n",
       "      <td>8.783835</td>\n",
       "      <td>9.680231</td>\n",
       "      <td>8.268755</td>\n",
       "      <td>9.827520</td>\n",
       "      <td>8.895333</td>\n",
       "      <td>8.399132</td>\n",
       "      <td>9.558890</td>\n",
       "      <td>7.797887</td>\n",
       "      <td>...</td>\n",
       "      <td>10.109839</td>\n",
       "      <td>8.129112</td>\n",
       "      <td>8.990020</td>\n",
       "      <td>8.409897</td>\n",
       "      <td>9.251225</td>\n",
       "      <td>8.685712</td>\n",
       "      <td>9.164466</td>\n",
       "      <td>8.595016</td>\n",
       "      <td>8.784922</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>10.289022</td>\n",
       "      <td>8.815002</td>\n",
       "      <td>8.574663</td>\n",
       "      <td>9.619898</td>\n",
       "      <td>8.465206</td>\n",
       "      <td>9.673041</td>\n",
       "      <td>8.824081</td>\n",
       "      <td>8.257421</td>\n",
       "      <td>9.393159</td>\n",
       "      <td>7.811494</td>\n",
       "      <td>...</td>\n",
       "      <td>10.509812</td>\n",
       "      <td>8.167474</td>\n",
       "      <td>9.085081</td>\n",
       "      <td>8.492114</td>\n",
       "      <td>8.766034</td>\n",
       "      <td>8.326267</td>\n",
       "      <td>8.972200</td>\n",
       "      <td>8.277074</td>\n",
       "      <td>8.914538</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>287</th>\n",
       "      <td>10.443093</td>\n",
       "      <td>8.886114</td>\n",
       "      <td>8.761951</td>\n",
       "      <td>9.798333</td>\n",
       "      <td>8.262609</td>\n",
       "      <td>9.813212</td>\n",
       "      <td>8.984246</td>\n",
       "      <td>8.270182</td>\n",
       "      <td>9.403510</td>\n",
       "      <td>7.790942</td>\n",
       "      <td>...</td>\n",
       "      <td>10.265976</td>\n",
       "      <td>8.106213</td>\n",
       "      <td>9.036253</td>\n",
       "      <td>8.514579</td>\n",
       "      <td>8.819316</td>\n",
       "      <td>8.392213</td>\n",
       "      <td>9.151364</td>\n",
       "      <td>8.262410</td>\n",
       "      <td>8.972986</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>288</th>\n",
       "      <td>10.327574</td>\n",
       "      <td>8.886320</td>\n",
       "      <td>8.731006</td>\n",
       "      <td>9.709379</td>\n",
       "      <td>8.298635</td>\n",
       "      <td>9.813801</td>\n",
       "      <td>8.952147</td>\n",
       "      <td>8.470894</td>\n",
       "      <td>9.507957</td>\n",
       "      <td>7.834961</td>\n",
       "      <td>...</td>\n",
       "      <td>10.216570</td>\n",
       "      <td>8.106966</td>\n",
       "      <td>9.032146</td>\n",
       "      <td>8.510935</td>\n",
       "      <td>8.863640</td>\n",
       "      <td>8.697643</td>\n",
       "      <td>8.952026</td>\n",
       "      <td>8.444355</td>\n",
       "      <td>9.001902</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>289</th>\n",
       "      <td>10.289376</td>\n",
       "      <td>8.820926</td>\n",
       "      <td>8.689403</td>\n",
       "      <td>9.670519</td>\n",
       "      <td>8.018137</td>\n",
       "      <td>9.876123</td>\n",
       "      <td>8.851283</td>\n",
       "      <td>8.388222</td>\n",
       "      <td>9.643753</td>\n",
       "      <td>7.686061</td>\n",
       "      <td>...</td>\n",
       "      <td>10.009546</td>\n",
       "      <td>7.911680</td>\n",
       "      <td>9.035404</td>\n",
       "      <td>8.423019</td>\n",
       "      <td>9.012495</td>\n",
       "      <td>8.843158</td>\n",
       "      <td>9.003032</td>\n",
       "      <td>8.235528</td>\n",
       "      <td>8.755377</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>778 rows × 13237 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          DDR1      RFC2     HSPA6       PAX8    GUCA1A    MIR5193      THRA   \n",
       "0    11.828700  9.523877  7.907263  10.750475  6.478643   8.698516  7.526617  \\\n",
       "1    13.241262  8.543289  8.435700  11.931321  7.151195  10.130569  9.457966   \n",
       "2    12.993611  8.004977  8.741661  11.579397  7.433843   9.621802  8.319854   \n",
       "3    12.028597  8.889190  7.975727  10.501003  7.320102   9.473202  8.133202   \n",
       "4    12.275150  7.982162  7.971315  11.017791  6.932531   9.590634  8.446712   \n",
       "..         ...       ...       ...        ...       ...        ...       ...   \n",
       "285  10.501408  9.016302  8.783835   9.680231  8.268755   9.827520  8.895333   \n",
       "286  10.289022  8.815002  8.574663   9.619898  8.465206   9.673041  8.824081   \n",
       "287  10.443093  8.886114  8.761951   9.798333  8.262609   9.813212  8.984246   \n",
       "288  10.327574  8.886320  8.731006   9.709379  8.298635   9.813801  8.952147   \n",
       "289  10.289376  8.820926  8.689403   9.670519  8.018137   9.876123  8.851283   \n",
       "\n",
       "       PTPN21       CCL5    CYP2E1  ...  MINOS1-NBL1    NUS1P3  MROH7-TTC4   \n",
       "0    7.276040  10.399704  6.642930  ...    11.356547  6.894852    9.188312  \\\n",
       "1    7.663284   6.242512  6.902703  ...    11.920599  1.356235    8.111887   \n",
       "2    7.718079   5.457564  6.741639  ...    10.215395  5.618485    9.432083   \n",
       "3    5.137284   8.062176  6.796330  ...    10.992724  6.310822    9.204276   \n",
       "4    5.407080  10.304378  7.145151  ...    10.871971  5.364275    9.003275   \n",
       "..        ...        ...       ...  ...          ...       ...         ...   \n",
       "285  8.399132   9.558890  7.797887  ...    10.109839  8.129112    8.990020   \n",
       "286  8.257421   9.393159  7.811494  ...    10.509812  8.167474    9.085081   \n",
       "287  8.270182   9.403510  7.790942  ...    10.265976  8.106213    9.036253   \n",
       "288  8.470894   9.507957  7.834961  ...    10.216570  8.106966    9.032146   \n",
       "289  8.388222   9.643753  7.686061  ...    10.009546  7.911680    9.035404   \n",
       "\n",
       "       RBM12B  LOC102725263   FAM86B1    SNHG17  LOC100505915    NPEPL1  group  \n",
       "0    8.628715      9.029101  8.669479  8.573230      5.535360  7.364660    0.0  \n",
       "1    8.679902      9.154389  8.743268  9.011958      4.717732  9.323271    0.0  \n",
       "2    7.999077      8.554622  9.404803  7.887245      7.180957  8.758899    0.0  \n",
       "3    9.277398     10.488880  8.439994  9.210394      5.208338  7.218112    0.0  \n",
       "4    8.045155      9.305313  8.852204  9.044612      6.692939  7.591082    0.0  \n",
       "..        ...           ...       ...       ...           ...       ...    ...  \n",
       "285  8.409897      9.251225  8.685712  9.164466      8.595016  8.784922    0.0  \n",
       "286  8.492114      8.766034  8.326267  8.972200      8.277074  8.914538    0.0  \n",
       "287  8.514579      8.819316  8.392213  9.151364      8.262410  8.972986    0.0  \n",
       "288  8.510935      8.863640  8.697643  8.952026      8.444355  9.001902    0.0  \n",
       "289  8.423019      9.012495  8.843158  9.003032      8.235528  8.755377    0.0  \n",
       "\n",
       "[778 rows x 13237 columns]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_GSE25066_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9c8aefb8-8e41-452b-bc95-c4835a0ed470",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_GSE25066_df.to_csv('gan_GSE25066.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "b43b1abf-970d-4993-9fab-fa6b0e388626",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_GSE25066_pCR = new_GSE25066_df[new_GSE25066_df['group'] == 0].drop('group',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "09d2cd00-0ce3-4ce7-82b2-28c5911b0651",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>gene_name</th>\n",
       "      <th>GSM615099</th>\n",
       "      <th>GSM615110</th>\n",
       "      <th>GSM615122</th>\n",
       "      <th>GSM615123</th>\n",
       "      <th>GSM615137</th>\n",
       "      <th>GSM615139</th>\n",
       "      <th>GSM615140</th>\n",
       "      <th>GSM615143</th>\n",
       "      <th>GSM615146</th>\n",
       "      <th>...</th>\n",
       "      <th>GSM615775</th>\n",
       "      <th>GSM615779</th>\n",
       "      <th>GSM615780</th>\n",
       "      <th>GSM615782</th>\n",
       "      <th>GSM615793</th>\n",
       "      <th>GSM615798</th>\n",
       "      <th>GSM615802</th>\n",
       "      <th>GSM615803</th>\n",
       "      <th>GSM615822</th>\n",
       "      <th>GSM615824</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DDR1</td>\n",
       "      <td>11.828700</td>\n",
       "      <td>13.241262</td>\n",
       "      <td>12.993611</td>\n",
       "      <td>12.028597</td>\n",
       "      <td>12.275150</td>\n",
       "      <td>12.313212</td>\n",
       "      <td>12.576470</td>\n",
       "      <td>12.205089</td>\n",
       "      <td>11.714574</td>\n",
       "      <td>...</td>\n",
       "      <td>11.400452</td>\n",
       "      <td>11.965518</td>\n",
       "      <td>11.383388</td>\n",
       "      <td>11.202747</td>\n",
       "      <td>12.591981</td>\n",
       "      <td>12.266062</td>\n",
       "      <td>11.559467</td>\n",
       "      <td>12.123890</td>\n",
       "      <td>12.471186</td>\n",
       "      <td>11.900511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>RFC2</td>\n",
       "      <td>9.523877</td>\n",
       "      <td>8.543289</td>\n",
       "      <td>8.004977</td>\n",
       "      <td>8.889190</td>\n",
       "      <td>7.982162</td>\n",
       "      <td>8.163444</td>\n",
       "      <td>7.916820</td>\n",
       "      <td>7.414843</td>\n",
       "      <td>8.380203</td>\n",
       "      <td>...</td>\n",
       "      <td>8.550332</td>\n",
       "      <td>8.442084</td>\n",
       "      <td>8.425894</td>\n",
       "      <td>8.053900</td>\n",
       "      <td>8.951475</td>\n",
       "      <td>9.241684</td>\n",
       "      <td>8.769740</td>\n",
       "      <td>9.368690</td>\n",
       "      <td>8.848875</td>\n",
       "      <td>7.908133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>HSPA6</td>\n",
       "      <td>7.907263</td>\n",
       "      <td>8.435700</td>\n",
       "      <td>8.741661</td>\n",
       "      <td>7.975727</td>\n",
       "      <td>7.971315</td>\n",
       "      <td>8.757782</td>\n",
       "      <td>9.215672</td>\n",
       "      <td>8.072444</td>\n",
       "      <td>7.661083</td>\n",
       "      <td>...</td>\n",
       "      <td>8.216528</td>\n",
       "      <td>7.794136</td>\n",
       "      <td>8.272750</td>\n",
       "      <td>8.701260</td>\n",
       "      <td>7.726220</td>\n",
       "      <td>8.109099</td>\n",
       "      <td>7.720345</td>\n",
       "      <td>8.124811</td>\n",
       "      <td>7.569580</td>\n",
       "      <td>8.669415</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>PAX8</td>\n",
       "      <td>10.750475</td>\n",
       "      <td>11.931321</td>\n",
       "      <td>11.579397</td>\n",
       "      <td>10.501003</td>\n",
       "      <td>11.017791</td>\n",
       "      <td>10.735321</td>\n",
       "      <td>11.066361</td>\n",
       "      <td>11.345411</td>\n",
       "      <td>10.635503</td>\n",
       "      <td>...</td>\n",
       "      <td>10.267392</td>\n",
       "      <td>10.536505</td>\n",
       "      <td>10.527342</td>\n",
       "      <td>11.137687</td>\n",
       "      <td>10.505399</td>\n",
       "      <td>11.196548</td>\n",
       "      <td>10.309222</td>\n",
       "      <td>10.881557</td>\n",
       "      <td>10.452863</td>\n",
       "      <td>11.107900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>GUCA1A</td>\n",
       "      <td>6.478643</td>\n",
       "      <td>7.151195</td>\n",
       "      <td>7.433843</td>\n",
       "      <td>7.320102</td>\n",
       "      <td>6.932531</td>\n",
       "      <td>7.479115</td>\n",
       "      <td>7.995924</td>\n",
       "      <td>6.886549</td>\n",
       "      <td>6.891103</td>\n",
       "      <td>...</td>\n",
       "      <td>7.100183</td>\n",
       "      <td>6.186551</td>\n",
       "      <td>6.288648</td>\n",
       "      <td>6.973356</td>\n",
       "      <td>6.502955</td>\n",
       "      <td>7.338256</td>\n",
       "      <td>6.762156</td>\n",
       "      <td>7.114815</td>\n",
       "      <td>7.006652</td>\n",
       "      <td>7.381252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13231</th>\n",
       "      <td>LOC102725263</td>\n",
       "      <td>9.029101</td>\n",
       "      <td>9.154389</td>\n",
       "      <td>8.554622</td>\n",
       "      <td>10.488880</td>\n",
       "      <td>9.305313</td>\n",
       "      <td>8.863490</td>\n",
       "      <td>9.031884</td>\n",
       "      <td>10.985185</td>\n",
       "      <td>8.159242</td>\n",
       "      <td>...</td>\n",
       "      <td>9.025092</td>\n",
       "      <td>9.003402</td>\n",
       "      <td>8.057661</td>\n",
       "      <td>8.880101</td>\n",
       "      <td>8.521056</td>\n",
       "      <td>10.591930</td>\n",
       "      <td>9.048223</td>\n",
       "      <td>9.584406</td>\n",
       "      <td>9.236633</td>\n",
       "      <td>9.360416</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13232</th>\n",
       "      <td>FAM86B1</td>\n",
       "      <td>8.669479</td>\n",
       "      <td>8.743268</td>\n",
       "      <td>9.404803</td>\n",
       "      <td>8.439994</td>\n",
       "      <td>8.852204</td>\n",
       "      <td>7.628408</td>\n",
       "      <td>7.984313</td>\n",
       "      <td>8.763259</td>\n",
       "      <td>8.467488</td>\n",
       "      <td>...</td>\n",
       "      <td>8.650094</td>\n",
       "      <td>8.681698</td>\n",
       "      <td>8.124114</td>\n",
       "      <td>8.650458</td>\n",
       "      <td>9.090492</td>\n",
       "      <td>8.196299</td>\n",
       "      <td>8.145812</td>\n",
       "      <td>8.930901</td>\n",
       "      <td>8.702834</td>\n",
       "      <td>8.380616</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13233</th>\n",
       "      <td>SNHG17</td>\n",
       "      <td>8.573230</td>\n",
       "      <td>9.011958</td>\n",
       "      <td>7.887245</td>\n",
       "      <td>9.210394</td>\n",
       "      <td>9.044612</td>\n",
       "      <td>8.507519</td>\n",
       "      <td>9.539290</td>\n",
       "      <td>9.337777</td>\n",
       "      <td>8.359296</td>\n",
       "      <td>...</td>\n",
       "      <td>8.825160</td>\n",
       "      <td>8.147968</td>\n",
       "      <td>9.106360</td>\n",
       "      <td>8.949937</td>\n",
       "      <td>8.426192</td>\n",
       "      <td>9.314493</td>\n",
       "      <td>8.131871</td>\n",
       "      <td>8.663448</td>\n",
       "      <td>8.823441</td>\n",
       "      <td>8.602202</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13234</th>\n",
       "      <td>LOC100505915</td>\n",
       "      <td>5.535360</td>\n",
       "      <td>4.717732</td>\n",
       "      <td>7.180957</td>\n",
       "      <td>5.208338</td>\n",
       "      <td>6.692939</td>\n",
       "      <td>7.099980</td>\n",
       "      <td>4.862343</td>\n",
       "      <td>4.648788</td>\n",
       "      <td>6.277681</td>\n",
       "      <td>...</td>\n",
       "      <td>5.794316</td>\n",
       "      <td>4.617974</td>\n",
       "      <td>6.251629</td>\n",
       "      <td>4.892419</td>\n",
       "      <td>6.506186</td>\n",
       "      <td>8.032424</td>\n",
       "      <td>7.643098</td>\n",
       "      <td>4.948300</td>\n",
       "      <td>4.216281</td>\n",
       "      <td>4.611442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13235</th>\n",
       "      <td>NPEPL1</td>\n",
       "      <td>7.364660</td>\n",
       "      <td>9.323271</td>\n",
       "      <td>8.758899</td>\n",
       "      <td>7.218112</td>\n",
       "      <td>7.591082</td>\n",
       "      <td>8.599020</td>\n",
       "      <td>8.545118</td>\n",
       "      <td>9.329027</td>\n",
       "      <td>7.245990</td>\n",
       "      <td>...</td>\n",
       "      <td>8.263459</td>\n",
       "      <td>7.577471</td>\n",
       "      <td>7.533360</td>\n",
       "      <td>8.136841</td>\n",
       "      <td>7.570360</td>\n",
       "      <td>7.903017</td>\n",
       "      <td>8.403836</td>\n",
       "      <td>7.302553</td>\n",
       "      <td>7.583528</td>\n",
       "      <td>8.396856</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13236 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          gene_name  GSM615099  GSM615110  GSM615122  GSM615123  GSM615137   \n",
       "0              DDR1  11.828700  13.241262  12.993611  12.028597  12.275150  \\\n",
       "1              RFC2   9.523877   8.543289   8.004977   8.889190   7.982162   \n",
       "2             HSPA6   7.907263   8.435700   8.741661   7.975727   7.971315   \n",
       "3              PAX8  10.750475  11.931321  11.579397  10.501003  11.017791   \n",
       "4            GUCA1A   6.478643   7.151195   7.433843   7.320102   6.932531   \n",
       "...             ...        ...        ...        ...        ...        ...   \n",
       "13231  LOC102725263   9.029101   9.154389   8.554622  10.488880   9.305313   \n",
       "13232       FAM86B1   8.669479   8.743268   9.404803   8.439994   8.852204   \n",
       "13233        SNHG17   8.573230   9.011958   7.887245   9.210394   9.044612   \n",
       "13234  LOC100505915   5.535360   4.717732   7.180957   5.208338   6.692939   \n",
       "13235        NPEPL1   7.364660   9.323271   8.758899   7.218112   7.591082   \n",
       "\n",
       "       GSM615139  GSM615140  GSM615143  GSM615146  ...  GSM615775  GSM615779   \n",
       "0      12.313212  12.576470  12.205089  11.714574  ...  11.400452  11.965518  \\\n",
       "1       8.163444   7.916820   7.414843   8.380203  ...   8.550332   8.442084   \n",
       "2       8.757782   9.215672   8.072444   7.661083  ...   8.216528   7.794136   \n",
       "3      10.735321  11.066361  11.345411  10.635503  ...  10.267392  10.536505   \n",
       "4       7.479115   7.995924   6.886549   6.891103  ...   7.100183   6.186551   \n",
       "...          ...        ...        ...        ...  ...        ...        ...   \n",
       "13231   8.863490   9.031884  10.985185   8.159242  ...   9.025092   9.003402   \n",
       "13232   7.628408   7.984313   8.763259   8.467488  ...   8.650094   8.681698   \n",
       "13233   8.507519   9.539290   9.337777   8.359296  ...   8.825160   8.147968   \n",
       "13234   7.099980   4.862343   4.648788   6.277681  ...   5.794316   4.617974   \n",
       "13235   8.599020   8.545118   9.329027   7.245990  ...   8.263459   7.577471   \n",
       "\n",
       "       GSM615780  GSM615782  GSM615793  GSM615798  GSM615802  GSM615803   \n",
       "0      11.383388  11.202747  12.591981  12.266062  11.559467  12.123890  \\\n",
       "1       8.425894   8.053900   8.951475   9.241684   8.769740   9.368690   \n",
       "2       8.272750   8.701260   7.726220   8.109099   7.720345   8.124811   \n",
       "3      10.527342  11.137687  10.505399  11.196548  10.309222  10.881557   \n",
       "4       6.288648   6.973356   6.502955   7.338256   6.762156   7.114815   \n",
       "...          ...        ...        ...        ...        ...        ...   \n",
       "13231   8.057661   8.880101   8.521056  10.591930   9.048223   9.584406   \n",
       "13232   8.124114   8.650458   9.090492   8.196299   8.145812   8.930901   \n",
       "13233   9.106360   8.949937   8.426192   9.314493   8.131871   8.663448   \n",
       "13234   6.251629   4.892419   6.506186   8.032424   7.643098   4.948300   \n",
       "13235   7.533360   8.136841   7.570360   7.903017   8.403836   7.302553   \n",
       "\n",
       "       GSM615822  GSM615824  \n",
       "0      12.471186  11.900511  \n",
       "1       8.848875   7.908133  \n",
       "2       7.569580   8.669415  \n",
       "3      10.452863  11.107900  \n",
       "4       7.006652   7.381252  \n",
       "...          ...        ...  \n",
       "13231   9.236633   9.360416  \n",
       "13232   8.702834   8.380616  \n",
       "13233   8.823441   8.602202  \n",
       "13234   4.216281   4.611442  \n",
       "13235   7.583528   8.396856  \n",
       "\n",
       "[13236 rows x 100 columns]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "GSE25066_pCR_df = pd.read_csv('GSE25066_pCR.csv')\n",
    "GSE25066_pCR_df.columns.values[0] = 'gene_name'\n",
    "GSE25066_pCR_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ea94f619-19ad-4dfe-bd16-2f7f12c909df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用reset_index方法重新排序索引\n",
    "gan_GSE25066_pCR = gan_GSE25066_pCR.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "78a6702d-7287-4d3f-936d-247cffcea0c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GSM0</th>\n",
       "      <th>GSM1</th>\n",
       "      <th>GSM2</th>\n",
       "      <th>GSM3</th>\n",
       "      <th>GSM4</th>\n",
       "      <th>GSM5</th>\n",
       "      <th>GSM6</th>\n",
       "      <th>GSM7</th>\n",
       "      <th>GSM8</th>\n",
       "      <th>GSM9</th>\n",
       "      <th>...</th>\n",
       "      <th>GSM379</th>\n",
       "      <th>GSM380</th>\n",
       "      <th>GSM381</th>\n",
       "      <th>GSM382</th>\n",
       "      <th>GSM383</th>\n",
       "      <th>GSM384</th>\n",
       "      <th>GSM385</th>\n",
       "      <th>GSM386</th>\n",
       "      <th>GSM387</th>\n",
       "      <th>GSM388</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>DDR1</th>\n",
       "      <td>11.828700</td>\n",
       "      <td>13.241262</td>\n",
       "      <td>12.993611</td>\n",
       "      <td>12.028597</td>\n",
       "      <td>12.275150</td>\n",
       "      <td>12.313212</td>\n",
       "      <td>12.576470</td>\n",
       "      <td>12.205089</td>\n",
       "      <td>11.714574</td>\n",
       "      <td>11.861684</td>\n",
       "      <td>...</td>\n",
       "      <td>10.253498</td>\n",
       "      <td>10.283988</td>\n",
       "      <td>10.608132</td>\n",
       "      <td>10.496068</td>\n",
       "      <td>10.450909</td>\n",
       "      <td>10.501408</td>\n",
       "      <td>10.289022</td>\n",
       "      <td>10.443093</td>\n",
       "      <td>10.327574</td>\n",
       "      <td>10.289376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>RFC2</th>\n",
       "      <td>9.523877</td>\n",
       "      <td>8.543289</td>\n",
       "      <td>8.004977</td>\n",
       "      <td>8.889190</td>\n",
       "      <td>7.982162</td>\n",
       "      <td>8.163444</td>\n",
       "      <td>7.916820</td>\n",
       "      <td>7.414843</td>\n",
       "      <td>8.380203</td>\n",
       "      <td>9.088528</td>\n",
       "      <td>...</td>\n",
       "      <td>9.082888</td>\n",
       "      <td>8.876357</td>\n",
       "      <td>8.815203</td>\n",
       "      <td>8.749627</td>\n",
       "      <td>8.853852</td>\n",
       "      <td>9.016302</td>\n",
       "      <td>8.815002</td>\n",
       "      <td>8.886114</td>\n",
       "      <td>8.886320</td>\n",
       "      <td>8.820926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HSPA6</th>\n",
       "      <td>7.907263</td>\n",
       "      <td>8.435700</td>\n",
       "      <td>8.741661</td>\n",
       "      <td>7.975727</td>\n",
       "      <td>7.971315</td>\n",
       "      <td>8.757782</td>\n",
       "      <td>9.215672</td>\n",
       "      <td>8.072444</td>\n",
       "      <td>7.661083</td>\n",
       "      <td>7.861307</td>\n",
       "      <td>...</td>\n",
       "      <td>8.726858</td>\n",
       "      <td>8.599212</td>\n",
       "      <td>8.489453</td>\n",
       "      <td>8.678428</td>\n",
       "      <td>8.578940</td>\n",
       "      <td>8.783835</td>\n",
       "      <td>8.574663</td>\n",
       "      <td>8.761951</td>\n",
       "      <td>8.731006</td>\n",
       "      <td>8.689403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PAX8</th>\n",
       "      <td>10.750475</td>\n",
       "      <td>11.931321</td>\n",
       "      <td>11.579397</td>\n",
       "      <td>10.501003</td>\n",
       "      <td>11.017791</td>\n",
       "      <td>10.735321</td>\n",
       "      <td>11.066361</td>\n",
       "      <td>11.345411</td>\n",
       "      <td>10.635503</td>\n",
       "      <td>10.659652</td>\n",
       "      <td>...</td>\n",
       "      <td>9.610138</td>\n",
       "      <td>9.764252</td>\n",
       "      <td>9.736207</td>\n",
       "      <td>9.511065</td>\n",
       "      <td>9.531338</td>\n",
       "      <td>9.680231</td>\n",
       "      <td>9.619898</td>\n",
       "      <td>9.798333</td>\n",
       "      <td>9.709379</td>\n",
       "      <td>9.670519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>GUCA1A</th>\n",
       "      <td>6.478643</td>\n",
       "      <td>7.151195</td>\n",
       "      <td>7.433843</td>\n",
       "      <td>7.320102</td>\n",
       "      <td>6.932531</td>\n",
       "      <td>7.479115</td>\n",
       "      <td>7.995924</td>\n",
       "      <td>6.886549</td>\n",
       "      <td>6.891103</td>\n",
       "      <td>6.887889</td>\n",
       "      <td>...</td>\n",
       "      <td>8.282845</td>\n",
       "      <td>8.467445</td>\n",
       "      <td>8.504788</td>\n",
       "      <td>8.248045</td>\n",
       "      <td>8.300835</td>\n",
       "      <td>8.268755</td>\n",
       "      <td>8.465206</td>\n",
       "      <td>8.262609</td>\n",
       "      <td>8.298635</td>\n",
       "      <td>8.018137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC102725263</th>\n",
       "      <td>9.029101</td>\n",
       "      <td>9.154389</td>\n",
       "      <td>8.554622</td>\n",
       "      <td>10.488880</td>\n",
       "      <td>9.305313</td>\n",
       "      <td>8.863490</td>\n",
       "      <td>9.031884</td>\n",
       "      <td>10.985185</td>\n",
       "      <td>8.159242</td>\n",
       "      <td>9.533490</td>\n",
       "      <td>...</td>\n",
       "      <td>9.013741</td>\n",
       "      <td>8.846117</td>\n",
       "      <td>8.986135</td>\n",
       "      <td>8.953793</td>\n",
       "      <td>8.901990</td>\n",
       "      <td>9.251225</td>\n",
       "      <td>8.766034</td>\n",
       "      <td>8.819316</td>\n",
       "      <td>8.863640</td>\n",
       "      <td>9.012495</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>FAM86B1</th>\n",
       "      <td>8.669479</td>\n",
       "      <td>8.743268</td>\n",
       "      <td>9.404803</td>\n",
       "      <td>8.439994</td>\n",
       "      <td>8.852204</td>\n",
       "      <td>7.628408</td>\n",
       "      <td>7.984313</td>\n",
       "      <td>8.763259</td>\n",
       "      <td>8.467488</td>\n",
       "      <td>8.693230</td>\n",
       "      <td>...</td>\n",
       "      <td>8.496355</td>\n",
       "      <td>8.602278</td>\n",
       "      <td>8.312710</td>\n",
       "      <td>8.225665</td>\n",
       "      <td>8.451160</td>\n",
       "      <td>8.685712</td>\n",
       "      <td>8.326267</td>\n",
       "      <td>8.392213</td>\n",
       "      <td>8.697643</td>\n",
       "      <td>8.843158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SNHG17</th>\n",
       "      <td>8.573230</td>\n",
       "      <td>9.011958</td>\n",
       "      <td>7.887245</td>\n",
       "      <td>9.210394</td>\n",
       "      <td>9.044612</td>\n",
       "      <td>8.507519</td>\n",
       "      <td>9.539290</td>\n",
       "      <td>9.337777</td>\n",
       "      <td>8.359296</td>\n",
       "      <td>8.182982</td>\n",
       "      <td>...</td>\n",
       "      <td>9.027749</td>\n",
       "      <td>8.833577</td>\n",
       "      <td>9.129616</td>\n",
       "      <td>9.035217</td>\n",
       "      <td>8.990586</td>\n",
       "      <td>9.164466</td>\n",
       "      <td>8.972200</td>\n",
       "      <td>9.151364</td>\n",
       "      <td>8.952026</td>\n",
       "      <td>9.003032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>LOC100505915</th>\n",
       "      <td>5.535360</td>\n",
       "      <td>4.717732</td>\n",
       "      <td>7.180957</td>\n",
       "      <td>5.208338</td>\n",
       "      <td>6.692939</td>\n",
       "      <td>7.099980</td>\n",
       "      <td>4.862343</td>\n",
       "      <td>4.648788</td>\n",
       "      <td>6.277681</td>\n",
       "      <td>5.648270</td>\n",
       "      <td>...</td>\n",
       "      <td>8.488754</td>\n",
       "      <td>8.453042</td>\n",
       "      <td>8.331416</td>\n",
       "      <td>8.602941</td>\n",
       "      <td>8.268628</td>\n",
       "      <td>8.595016</td>\n",
       "      <td>8.277074</td>\n",
       "      <td>8.262410</td>\n",
       "      <td>8.444355</td>\n",
       "      <td>8.235528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>NPEPL1</th>\n",
       "      <td>7.364660</td>\n",
       "      <td>9.323271</td>\n",
       "      <td>8.758899</td>\n",
       "      <td>7.218112</td>\n",
       "      <td>7.591082</td>\n",
       "      <td>8.599020</td>\n",
       "      <td>8.545118</td>\n",
       "      <td>9.329027</td>\n",
       "      <td>7.245990</td>\n",
       "      <td>8.819088</td>\n",
       "      <td>...</td>\n",
       "      <td>9.088895</td>\n",
       "      <td>9.010774</td>\n",
       "      <td>8.802259</td>\n",
       "      <td>8.721118</td>\n",
       "      <td>8.795978</td>\n",
       "      <td>8.784922</td>\n",
       "      <td>8.914538</td>\n",
       "      <td>8.972986</td>\n",
       "      <td>9.001902</td>\n",
       "      <td>8.755377</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13236 rows × 389 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   GSM0       GSM1       GSM2       GSM3       GSM4   \n",
       "DDR1          11.828700  13.241262  12.993611  12.028597  12.275150  \\\n",
       "RFC2           9.523877   8.543289   8.004977   8.889190   7.982162   \n",
       "HSPA6          7.907263   8.435700   8.741661   7.975727   7.971315   \n",
       "PAX8          10.750475  11.931321  11.579397  10.501003  11.017791   \n",
       "GUCA1A         6.478643   7.151195   7.433843   7.320102   6.932531   \n",
       "...                 ...        ...        ...        ...        ...   \n",
       "LOC102725263   9.029101   9.154389   8.554622  10.488880   9.305313   \n",
       "FAM86B1        8.669479   8.743268   9.404803   8.439994   8.852204   \n",
       "SNHG17         8.573230   9.011958   7.887245   9.210394   9.044612   \n",
       "LOC100505915   5.535360   4.717732   7.180957   5.208338   6.692939   \n",
       "NPEPL1         7.364660   9.323271   8.758899   7.218112   7.591082   \n",
       "\n",
       "                   GSM5       GSM6       GSM7       GSM8       GSM9  ...   \n",
       "DDR1          12.313212  12.576470  12.205089  11.714574  11.861684  ...  \\\n",
       "RFC2           8.163444   7.916820   7.414843   8.380203   9.088528  ...   \n",
       "HSPA6          8.757782   9.215672   8.072444   7.661083   7.861307  ...   \n",
       "PAX8          10.735321  11.066361  11.345411  10.635503  10.659652  ...   \n",
       "GUCA1A         7.479115   7.995924   6.886549   6.891103   6.887889  ...   \n",
       "...                 ...        ...        ...        ...        ...  ...   \n",
       "LOC102725263   8.863490   9.031884  10.985185   8.159242   9.533490  ...   \n",
       "FAM86B1        7.628408   7.984313   8.763259   8.467488   8.693230  ...   \n",
       "SNHG17         8.507519   9.539290   9.337777   8.359296   8.182982  ...   \n",
       "LOC100505915   7.099980   4.862343   4.648788   6.277681   5.648270  ...   \n",
       "NPEPL1         8.599020   8.545118   9.329027   7.245990   8.819088  ...   \n",
       "\n",
       "                 GSM379     GSM380     GSM381     GSM382     GSM383   \n",
       "DDR1          10.253498  10.283988  10.608132  10.496068  10.450909  \\\n",
       "RFC2           9.082888   8.876357   8.815203   8.749627   8.853852   \n",
       "HSPA6          8.726858   8.599212   8.489453   8.678428   8.578940   \n",
       "PAX8           9.610138   9.764252   9.736207   9.511065   9.531338   \n",
       "GUCA1A         8.282845   8.467445   8.504788   8.248045   8.300835   \n",
       "...                 ...        ...        ...        ...        ...   \n",
       "LOC102725263   9.013741   8.846117   8.986135   8.953793   8.901990   \n",
       "FAM86B1        8.496355   8.602278   8.312710   8.225665   8.451160   \n",
       "SNHG17         9.027749   8.833577   9.129616   9.035217   8.990586   \n",
       "LOC100505915   8.488754   8.453042   8.331416   8.602941   8.268628   \n",
       "NPEPL1         9.088895   9.010774   8.802259   8.721118   8.795978   \n",
       "\n",
       "                 GSM384     GSM385     GSM386     GSM387     GSM388  \n",
       "DDR1          10.501408  10.289022  10.443093  10.327574  10.289376  \n",
       "RFC2           9.016302   8.815002   8.886114   8.886320   8.820926  \n",
       "HSPA6          8.783835   8.574663   8.761951   8.731006   8.689403  \n",
       "PAX8           9.680231   9.619898   9.798333   9.709379   9.670519  \n",
       "GUCA1A         8.268755   8.465206   8.262609   8.298635   8.018137  \n",
       "...                 ...        ...        ...        ...        ...  \n",
       "LOC102725263   9.251225   8.766034   8.819316   8.863640   9.012495  \n",
       "FAM86B1        8.685712   8.326267   8.392213   8.697643   8.843158  \n",
       "SNHG17         9.164466   8.972200   9.151364   8.952026   9.003032  \n",
       "LOC100505915   8.595016   8.277074   8.262410   8.444355   8.235528  \n",
       "NPEPL1         8.784922   8.914538   8.972986   9.001902   8.755377  \n",
       "\n",
       "[13236 rows x 389 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 使用rename方法修改列名\n",
    "gan_GSE25066_pCR = gan_GSE25066_pCR.T.rename(columns=lambda x: 'GSM' + str(x))\n",
    "gan_GSE25066_pCR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "7ec01ac2-a5f3-4610-993b-5827a9efa094",
   "metadata": {},
   "outputs": [],
   "source": [
    "gan_GSE25066_pCR.to_csv('gan_GSE25066_pCR.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
